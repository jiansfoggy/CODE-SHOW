{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O3nLqNGdeIwv"
   },
   "source": [
    "# Content\n",
    "- Introduction\n",
    "- Model\n",
    "- Result\n",
    "  - Logistic Regression\n",
    "  - PCA + LR, Retain 90% Eigenvalue\n",
    "  - PCA + LR, Retain 95% Eigenvalue\n",
    "  - Random Forest\n",
    "  - PCA + RF, Retain 90% Eigenvalue\n",
    "  - PCA + RF, Retain 95% Eigenvalue\n",
    "- Discussion\n",
    "- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcHcJ3HVeIww"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this paper, 2 machine learning methods (Logistic Regression and Random Forest) are implemented to classify the 10 categories (0-9) from MNIST dataset. For each method, we are not only use it to predict, but also we will try to predict after dimension reduction.  \n",
    "\n",
    "Finally, the results will be detailed compared. We will compare within the method, at the same time, we will compare the result across methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvggQCQqeIwx"
   },
   "source": [
    "# Model\n",
    "\n",
    "There is very detailed analysis and comparison within each methods.  \n",
    "\n",
    "For Logistic Regression(LR), we firstly code a LR class from scratch and do prediction. Then, we plan to do dimension deduction before classification and see what happens on this condition. Therefore, we seperately retain 90% and 95% eigenvalues at PCA part, and generate new dataset with related eigenvectors.\n",
    "\n",
    "For Random Forest(RF), we repeat the similar process. But this time, we call scikit-learn package to train random forest model.\n",
    "\n",
    "Additionally, the MNIST dataset contains 60000 images in training dataset. This kind of volume of dataset is a big burden for training LR model and RF model. To save time and protect our laptop, we keep 25000 training images for LR model and 20000 training images for RF models.\n",
    "\n",
    "This try can improve time-consuming problem, but will reduce the overall accuracy. Hence, we trust that the full training dataset will bring us the best prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBZC0MuVeIwx"
   },
   "source": [
    "# Result\n",
    "## Logistic Regression\n",
    "\n",
    "Here, we set learning rate as 0.00002, iteration time as 200001 and choose stochastic gradient descent to do iterataion.  \n",
    "For all categories, the cost is bigger than 300 for every iteration. Even if we have the high cost, we get good prediction result, the overall accuracy is 91.03%. The micro average accuracy is  0.910; the macro average is 0.909; the weighted average is 0.910."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ctla8IpkeIwy",
    "outputId": "efca5967-2703-4200-dfb6-8d935aa6bb97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 948    0    4    3    0   11    8    2    2    2]\n",
      " [   0 1109    3    2    1    1    4    1   13    1]\n",
      " [   7   12  909   28    9    4   10   12   38    3]\n",
      " [   4    3   20  913    4   20    3   13   22    8]\n",
      " [   1    1    5    6  910    0   10    4    9   36]\n",
      " [  13    4    7   36    8  765   11    6   36    6]\n",
      " [  10    3   13    0    4   22  902    1    3    0]\n",
      " [   4    8   20    4    7    4    1  932    6   42]\n",
      " [   9   11   10   23   14   28   15   15  836   13]\n",
      " [   8    9    1   18   37   14    0   30   13  879]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.967     0.956       980\n",
      "           1      0.956     0.977     0.966      1135\n",
      "           2      0.916     0.881     0.898      1032\n",
      "           3      0.884     0.904     0.894      1010\n",
      "           4      0.915     0.927     0.921       982\n",
      "           5      0.880     0.858     0.869       892\n",
      "           6      0.936     0.942     0.939       958\n",
      "           7      0.917     0.907     0.912      1028\n",
      "           8      0.855     0.858     0.857       974\n",
      "           9      0.888     0.871     0.879      1009\n",
      "\n",
      "   micro avg      0.910     0.910     0.910     10000\n",
      "   macro avg      0.909     0.909     0.909     10000\n",
      "weighted avg      0.910     0.910     0.910     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kWooiRueIw4"
   },
   "source": [
    "## PCA + Logistic Regression, Retain 90% Eigenvalues\n",
    "\n",
    "As we can see, we keep 86 eigenvectors, set learning rate as 0.00002, iteration time as 200001 and choose stochastic gradient descent to do iterataion.\n",
    "For all categories, the cost is bigger than 2000 for every iteration. The overall accuracy is 71.97%. The micro average accuracy is 0.720; the macro average is 0.713; the weighted average is 0.716."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "IxLlFCxveIw5",
    "outputId": "465f7973-00c4-43da-f812-2385d648566a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 819    0   14   37    4   28   51    2   22    3]\n",
      " [   1 1037   21   11   14    4    7    1   38    1]\n",
      " [  34   55  685   45   24   28   84    3   67    7]\n",
      " [  40   13   61  731   17   49   28   17   35   19]\n",
      " [   6   41   31   20  745   15   19   13   19   73]\n",
      " [  38   30   34  199   52  375   44    9   61   50]\n",
      " [  42   10   50    5   33    9  784    5   19    1]\n",
      " [   3   34   10   14   41    9    8  784   17  108]\n",
      " [  92   43   27   43   37   58   31   11  613   19]\n",
      " [  10    8    9   27   75   16    2  210   28  624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.755     0.836     0.793       980\n",
      "           1      0.816     0.914     0.862      1135\n",
      "           2      0.727     0.664     0.694      1032\n",
      "           3      0.646     0.724     0.683      1010\n",
      "           4      0.715     0.759     0.736       982\n",
      "           5      0.635     0.420     0.506       892\n",
      "           6      0.741     0.818     0.778       958\n",
      "           7      0.743     0.763     0.753      1028\n",
      "           8      0.667     0.629     0.648       974\n",
      "           9      0.690     0.618     0.652      1009\n",
      "\n",
      "   micro avg      0.720     0.720     0.720     10000\n",
      "   macro avg      0.713     0.714     0.710     10000\n",
      "weighted avg      0.716     0.720     0.714     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w1pVJrVUeIw7"
   },
   "source": [
    "## PCA + Logistic Regression, Retain 95% Eigenvalues\n",
    "\n",
    "As we can see, we keep 153 eigenvectors, set learning rate as 0.00002, iteration time as 200001 and choose stochastic gradient descent to do iterataion.\n",
    "For all categories, the cost is bigger than 1000 for every iteration. The overall accuracy is 87.75%. The micro average accuracy is 0.877; the macro average is 0.876; the weighted average is 0.877."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SkDVnnHeIw7",
    "outputId": "d1d6c0f4-4d19-4e56-fd65-4dc9a71df999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 932    0    2    5    1   13   17    1    8    1]\n",
      " [   0 1098    5    2    0    3    4    3   20    0]\n",
      " [  11   15  877   22   18    9   15    9   53    3]\n",
      " [  13    2   36  854    1   45   11   10   24   14]\n",
      " [   3    4   16    1  888    4   14    8    9   35]\n",
      " [  19    6   11   59   22  674   23   12   53   13]\n",
      " [  20    3    6    2   13   22  879    3   10    0]\n",
      " [   2   14   23    7   13    4    0  915    6   44]\n",
      " [  15   11   17   27   16   32   14   14  818   10]\n",
      " [   8    7    1   14   52    9    4   58   16  840]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.911     0.951     0.931       980\n",
      "           1      0.947     0.967     0.957      1135\n",
      "           2      0.882     0.850     0.866      1032\n",
      "           3      0.860     0.846     0.853      1010\n",
      "           4      0.867     0.904     0.885       982\n",
      "           5      0.827     0.756     0.790       892\n",
      "           6      0.896     0.918     0.907       958\n",
      "           7      0.886     0.890     0.888      1028\n",
      "           8      0.804     0.840     0.822       974\n",
      "           9      0.875     0.833     0.853      1009\n",
      "\n",
      "   micro avg      0.877     0.877     0.877     10000\n",
      "   macro avg      0.876     0.875     0.875     10000\n",
      "weighted avg      0.877     0.877     0.877     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3T0IrB4eIw-"
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Here, to save time, we only build 200 trees, but get good prediction result.\n",
    "\n",
    "As we can see, the Mean Squared Error(MSE) is 0.7315. The overall accuracy is 96.08%. Then we try different max_depth (3,7,10,13,16,20) to predict and see if the model has potential to improve and we find that none of them have accuracy higher than 96.08% Thus, the current model is the best one. And the micro average accuracy is  0.961; the macro average is 0.961; the weighted average is 0.961."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "LWuSo6SWeIw-",
    "outputId": "e79b898c-a217-4a0a-9a82-5aa37f6fe7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 968    0    0    0    0    4    4    1    3    0]\n",
      " [   0 1122    3    3    0    2    2    1    2    0]\n",
      " [   6    0  987   10    3    0    7   11    7    1]\n",
      " [   1    0   11  968    0   10    0    9    7    4]\n",
      " [   1    0    2    0  944    0    8    0    3   24]\n",
      " [   5    1    0   22    3  843    8    2    6    2]\n",
      " [  11    3    0    0    7    4  931    0    2    0]\n",
      " [   1    6   24    1    3    0    0  979    4   10]\n",
      " [   5    1    6    6    5    6    9    5  917   14]\n",
      " [   5    6    3   14   15    4    2    3    8  949]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.965     0.988     0.976       980\n",
      "           1      0.985     0.989     0.987      1135\n",
      "           2      0.953     0.956     0.955      1032\n",
      "           3      0.945     0.958     0.952      1010\n",
      "           4      0.963     0.961     0.962       982\n",
      "           5      0.966     0.945     0.955       892\n",
      "           6      0.959     0.972     0.965       958\n",
      "           7      0.968     0.952     0.960      1028\n",
      "           8      0.956     0.941     0.949       974\n",
      "           9      0.945     0.941     0.943      1009\n",
      "\n",
      "   micro avg      0.961     0.961     0.961     10000\n",
      "   macro avg      0.961     0.960     0.960     10000\n",
      "weighted avg      0.961     0.961     0.961     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKUlST/8fwmVdNhatAbcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YL9nqmpeeIxA"
   },
   "source": [
    "## PCA + Random Forest, Retain 90% Eigenvalues\n",
    "\n",
    "As we can see, we keep 86 eigenvectors, and the Mean Squared Error(MSE) is 3.2954. The overall accuracy is 79.12%. Then we try different max_depth (3,7,10,13,16,20) to predict and see if the model has potential to improve and we find that the accuracies from all the max_depths are below 79.05%. Thus, the current model is the best one. And the micro average accuracy is  0.790; the macro average is 0.789; the weighted average is 0.791."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "QoehvFFbeIxB",
    "outputId": "7af7787e-4107-46b6-9c67-e11d6a89da7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 848    0   16   13    2   12   56    0   33    0]\n",
      " [   0 1089   13    2    4    0   10    1   16    0]\n",
      " [  34    9  817   24    5    6   67    4   66    0]\n",
      " [  27    1   36  778   13   57   27   12   49   10]\n",
      " [   0   20   19    3  819    1   16   18    5   81]\n",
      " [  33    5   33  241   18  439   30    6   78    9]\n",
      " [  20    4   37    0   15   11  846    0   25    0]\n",
      " [   0   26    4    2   54    0    5  818   10  109]\n",
      " [  62   14   16   38    7   24   21    9  777    6]\n",
      " [   4    6   10   17  113    9    0  170    6  674]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.825     0.865     0.845       980\n",
      "           1      0.928     0.959     0.943      1135\n",
      "           2      0.816     0.792     0.804      1032\n",
      "           3      0.696     0.770     0.731      1010\n",
      "           4      0.780     0.834     0.806       982\n",
      "           5      0.785     0.492     0.605       892\n",
      "           6      0.785     0.883     0.831       958\n",
      "           7      0.788     0.796     0.792      1028\n",
      "           8      0.730     0.798     0.762       974\n",
      "           9      0.758     0.668     0.710      1009\n",
      "\n",
      "   micro avg      0.790     0.790     0.790     10000\n",
      "   macro avg      0.789     0.786     0.783     10000\n",
      "weighted avg      0.791     0.790     0.787     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npClJUkv/H9K7TN/6sNyNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3z1gYcNueIxD"
   },
   "source": [
    "## PCA + Random Forest, Retain 95% Eigenvalues\n",
    "\n",
    "As we can see, we keep 153 eigenvectors, and the Mean Squared Error(MSE) is 1.7849. The overall accuracy is 89.28%. Then we try different max_depth (3,7,10,13,16,20) to predict and see if the model has potential to improve and we find that the accuracy from all different max_depth is less than 89.28%. Thus, the current model is the best one. And the micro average accuracy is 0.893; the macro average is 0.892; the weighted average is  0.893."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "bDh5vyKgeIxD",
    "outputId": "00c718cf-126b-4b1b-9d62-133ad5460fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 910    0    8    3    0    3   45    1   10    0]\n",
      " [   0 1112    6    4    1    0    5    1    6    0]\n",
      " [  12    3  922   23    5    0   29    7   31    0]\n",
      " [  12    0   27  866    3   48   20   13   15    6]\n",
      " [   0    3   11    0  890    0   10    9    7   52]\n",
      " [  27    3    8   66   15  691   23    9   38   12]\n",
      " [  19    1   14    1   14    5  883    0   21    0]\n",
      " [   0   15   10    1   29    0    2  921    7   43]\n",
      " [  17    2   17   22    2   23   16    8  861    6]\n",
      " [   6    8    2   19   68    4    1   26    3  872]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.907     0.929     0.918       980\n",
      "           1      0.969     0.980     0.975      1135\n",
      "           2      0.900     0.893     0.896      1032\n",
      "           3      0.862     0.857     0.860      1010\n",
      "           4      0.867     0.906     0.886       982\n",
      "           5      0.893     0.775     0.830       892\n",
      "           6      0.854     0.922     0.887       958\n",
      "           7      0.926     0.896     0.911      1028\n",
      "           8      0.862     0.884     0.873       974\n",
      "           9      0.880     0.864     0.872      1009\n",
      "\n",
      "   micro avg      0.893     0.893     0.893     10000\n",
      "   macro avg      0.892     0.891     0.891     10000\n",
      "weighted avg      0.893     0.893     0.892     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntWTSlCSppf8PaTI0Kk03yGcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXJD8EdOeIxG"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "Compared with LR, RF performs apparently better. The following is accuracy table.\n",
    "\n",
    "Accuracy Table      | Basic Model | Retain 90% | Retain 95%\n",
    "------------------------------|----------------------|-------------------|--------------------\n",
    "Logistic Regression | 91.03% | 71.97% | 87.75%\n",
    "Random Forst           | 96.08% | 79.12% | 89.28%\n",
    "\n",
    "Meanwhile, RF takes less time than LR in training on our condition, which means that our scratched class has huge potential to improve the processing time as fast as matured package.\n",
    "\n",
    "Additionally, given that we use the partial dataset, it is reasonable to believe both models will bring about better prediction result if we train full dataset.\n",
    "\n",
    "Generally speaking, if the scale of dataset is proper and there are many factor variables, we will be inclined to random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzGk8vhieIxW"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lsp1cShprjtc"
   },
   "outputs": [],
   "source": [
    "# import dataset and seperate them as train set and test set\n",
    "# index x represents image, index y represents label\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "from numpy.linalg import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-oiCEOp9rtgG",
    "outputId": "a214e58c-f34e-40bc-de5f-1e24bd21e754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# download MNIST dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# convert data type to float 32\n",
    "x_train=x_train[0:25000,]\n",
    "y_train=y_train[0:25000,]\n",
    "x_train=np.float32(x_train)\n",
    "x_test=np.float32(x_test)\n",
    "x_train = x_train.reshape(np.shape(x_train)[0], 28*28)/255.0\n",
    "x_test = x_test.reshape(np.shape(x_test)[0], 28*28)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIaeqcX2ssHk"
   },
   "outputs": [],
   "source": [
    "# make sure the 10 classes\n",
    "label_dict = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4',\n",
    "              5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIqz73nxsCTJ"
   },
   "source": [
    "## Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AV9D66Mhz-Kk"
   },
   "outputs": [],
   "source": [
    "# build a LR class\n",
    "class SJLogis_Regre(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.x_train = X\n",
    "        self.y_train = Y\n",
    "        \n",
    "    def split_category1(self, category_name):\n",
    "        yy_train=[]\n",
    "        for i in range(len(y_train)):\n",
    "            if (self.y_train[i]==category_name):\n",
    "                yy_train.append(1)\n",
    "            else: yy_train.append(0)\n",
    "        return yy_train \n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "      \n",
    "    def log_likelihood1(self, ytrain_c, weight):\n",
    "        \n",
    "        # add intercept\n",
    "        intercept = np.ones((np.shape(self.x_train)[0], 1))\n",
    "        xtrain_c = np.hstack((intercept, self.x_train))\n",
    "        \n",
    "        weight = np.reshape(weight,(np.shape(xtrain_c)[1], 1))\n",
    "        a = np.dot(xtrain_c, weight)\n",
    "        \n",
    "        ll = np.sum( np.multiply(ytrain_c,a.T) - np.log(1+np.exp(a.T)) ) \n",
    "        return ll\n",
    "    \n",
    "    def gradient_descent1(self, ytrain_c, learning_rate, iteration_time):\n",
    "        \n",
    "        # add intercept\n",
    "        intercept = np.ones((np.shape(self.x_train)[0], 1))\n",
    "        xtrain_c = np.hstack((intercept, self.x_train))\n",
    "        \n",
    "        # initial weight\n",
    "        weight = np.zeros((1,np.shape(xtrain_c)[1]))\n",
    "        ytrain_c = np.reshape(ytrain_c,(1, np.shape(ytrain_c)[0]))\n",
    "        \n",
    "        # do iteration\n",
    "        for i in range(iteration_time):\n",
    "            a = np.dot(weight, xtrain_c.T)\n",
    "            pred = self.sigmoid(a)\n",
    "\n",
    "            diff = ytrain_c - pred\n",
    "            \n",
    "            gradient = np.dot(diff, xtrain_c)\n",
    "            weight = weight + learning_rate * gradient\n",
    "            \n",
    "            # Print the cost\n",
    "            if (i % 10000 == 0):\n",
    "                cost = -self.log_likelihood1(ytrain_c, weight)\n",
    "\n",
    "                print (\"the cost in %d step is\" %(i),cost)\n",
    "        \n",
    "        return weight\n",
    "      \n",
    "    def get_pcx(self, xtest_c, weight_c):\n",
    "        \n",
    "        add_intercept = np.hstack((1, xtest_c))\n",
    "        \n",
    "        p_c = np.dot(weight_c,add_intercept)\n",
    "        result_c = self.sigmoid(p_c)\n",
    "        return result_c\n",
    "      \n",
    "    def predict_c(self, total_result):\n",
    "        value = np.where(total_result == np.max(total_result))\n",
    "        return value[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMFPMxFv_x5z"
   },
   "outputs": [],
   "source": [
    "# split the train as 10 categories\n",
    "JS = SJLogis_Regre()\n",
    "JS.train(x_train,y_train)\n",
    "y0_train = JS.split_category1(0)\n",
    "y1_train = JS.split_category1(1)\n",
    "y2_train = JS.split_category1(2)\n",
    "y3_train = JS.split_category1(3)\n",
    "y4_train = JS.split_category1(4)\n",
    "y5_train = JS.split_category1(5)\n",
    "y6_train = JS.split_category1(6)\n",
    "y7_train = JS.split_category1(7)\n",
    "y8_train = JS.split_category1(8)\n",
    "y9_train = JS.split_category1(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptJh7F8BARPD",
    "outputId": "632b6972-4970-4cb3-e35e-93e3bcd51688",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cost in 0 step is 44580.57417373677\n",
      "the cost in 10000 step is 412.8080815512875\n",
      "the cost in 20000 step is 370.7995168731614\n",
      "the cost in 30000 step is 349.03500935521964\n",
      "the cost in 40000 step is 334.941829873847\n",
      "the cost in 50000 step is 324.77269404959617\n",
      "the cost in 60000 step is 316.92963141079167\n",
      "the cost in 70000 step is 310.5973263235666\n",
      "the cost in 80000 step is 305.3111083395682\n",
      "the cost in 90000 step is 300.78525776611934\n",
      "the cost in 100000 step is 296.83421246816937\n",
      "the cost in 110000 step is 293.3324977335265\n",
      "the cost in 120000 step is 290.1926229696758\n",
      "the cost in 130000 step is 287.3519579956931\n",
      "the cost in 140000 step is 284.7643650903752\n",
      "the cost in 150000 step is 282.3945779769092\n",
      "the cost in 160000 step is 280.2144309343582\n",
      "the cost in 170000 step is 278.20049895215016\n",
      "the cost in 180000 step is 276.3327746489955\n",
      "the cost in 190000 step is 274.5939815786974\n",
      "the cost in 200000 step is 272.96918935784726\n",
      "the cost in 0 step is 33080.59816744591\n",
      "the cost in 10000 step is 550.7954826482951\n",
      "the cost in 20000 step is 502.39294304266343\n",
      "the cost in 30000 step is 474.8371769083394\n",
      "the cost in 40000 step is 456.08750304100215\n",
      "the cost in 50000 step is 442.14216534290154\n",
      "the cost in 60000 step is 431.13918950006047\n",
      "the cost in 70000 step is 422.0898601260789\n",
      "the cost in 80000 step is 414.4224372127335\n",
      "the cost in 90000 step is 407.78254310020645\n",
      "the cost in 100000 step is 401.9377208418714\n",
      "the cost in 110000 step is 396.72813455169273\n",
      "the cost in 120000 step is 392.0391075465796\n",
      "the cost in 130000 step is 387.78481705223066\n",
      "the cost in 140000 step is 383.89844474536545\n",
      "the cost in 150000 step is 380.32642993846326\n",
      "the cost in 160000 step is 377.0251490963904\n",
      "the cost in 170000 step is 373.9588013151387\n",
      "the cost in 180000 step is 371.0978543060005\n",
      "the cost in 190000 step is 368.41781150373436\n",
      "the cost in 200000 step is 365.89822178999714\n",
      "the cost in 0 step is 44875.62057028671\n",
      "the cost in 10000 step is 1527.1935188878765\n",
      "the cost in 20000 step is 1489.7624360660413\n",
      "the cost in 30000 step is 1473.5812882240882\n",
      "the cost in 40000 step is 1464.28255546705\n",
      "the cost in 50000 step is 1458.0791150329392\n",
      "the cost in 60000 step is 1453.5394920102412\n",
      "the cost in 70000 step is 1450.004000446804\n",
      "the cost in 80000 step is 1447.1263735208063\n",
      "the cost in 90000 step is 1444.7070456540266\n",
      "the cost in 100000 step is 1442.6225515197996\n",
      "the cost in 110000 step is 1440.792128266716\n",
      "the cost in 120000 step is 1439.1605139974408\n",
      "the cost in 130000 step is 1437.6884669135852\n",
      "the cost in 140000 step is 1436.3472416472746\n",
      "the cost in 150000 step is 1435.115218222465\n",
      "the cost in 160000 step is 1433.975762001227\n",
      "the cost in 170000 step is 1432.9158182764052\n",
      "the cost in 180000 step is 1431.9249618118909\n",
      "the cost in 190000 step is 1430.994737384277\n",
      "the cost in 200000 step is 1430.1181918494487\n",
      "the cost in 0 step is 46327.867893498995\n",
      "the cost in 10000 step is 1816.2657462181696\n",
      "the cost in 20000 step is 1787.1868871628421\n",
      "the cost in 30000 step is 1772.430541748359\n",
      "the cost in 40000 step is 1762.813429003373\n",
      "the cost in 50000 step is 1755.8587026929795\n",
      "the cost in 60000 step is 1750.5340295258807\n",
      "the cost in 70000 step is 1746.3028148101757\n",
      "the cost in 80000 step is 1742.846746232482\n",
      "the cost in 90000 step is 1739.9610531599155\n",
      "the cost in 100000 step is 1737.507101823621\n",
      "the cost in 110000 step is 1735.3877013943652\n",
      "the cost in 120000 step is 1733.5328767750968\n",
      "the cost in 130000 step is 1731.891113113401\n",
      "the cost in 140000 step is 1730.4237316630456\n",
      "the cost in 150000 step is 1729.1011669498246\n",
      "the cost in 160000 step is 1727.9004444478055\n",
      "the cost in 170000 step is 1726.8034377948327\n",
      "the cost in 180000 step is 1725.7956434501432\n",
      "the cost in 190000 step is 1724.8653052741406\n",
      "the cost in 200000 step is 1724.0027796964225\n",
      "the cost in 0 step is 40505.38480764192\n",
      "the cost in 10000 step is 1058.9724320329392\n",
      "the cost in 20000 step is 1008.2451543645313\n",
      "the cost in 30000 step is 982.2594200311594\n",
      "the cost in 40000 step is 965.1306038814467\n",
      "the cost in 50000 step is 952.5801959991448\n",
      "the cost in 60000 step is 942.8508114006631\n",
      "the cost in 70000 step is 935.0358179852907\n",
      "the cost in 80000 step is 928.5965332335497\n",
      "the cost in 90000 step is 923.1839465907186\n",
      "the cost in 100000 step is 918.5596481128854\n",
      "the cost in 110000 step is 914.5547658067544\n",
      "the cost in 120000 step is 911.046219268383\n",
      "the cost in 130000 step is 907.9421077669203\n",
      "the cost in 140000 step is 905.1723554856202\n",
      "the cost in 150000 step is 902.6825363239252\n",
      "the cost in 160000 step is 900.4296887680754\n",
      "the cost in 170000 step is 898.379411905431\n",
      "the cost in 180000 step is 896.503807090837\n",
      "the cost in 190000 step is 894.77999067092\n",
      "the cost in 200000 step is 893.1890004609638\n",
      "the cost in 0 step is 39654.51584231053\n",
      "the cost in 10000 step is 1739.7685373162744\n",
      "the cost in 20000 step is 1687.2992591581701\n",
      "the cost in 30000 step is 1662.5430717394986\n",
      "the cost in 40000 step is 1647.091160348093\n",
      "the cost in 50000 step is 1636.080763112063\n",
      "the cost in 60000 step is 1627.651831524264\n",
      "the cost in 70000 step is 1620.9133229460733\n",
      "the cost in 80000 step is 1615.3647105055325\n",
      "the cost in 90000 step is 1610.6916363130424\n",
      "the cost in 100000 step is 1606.6815212279575\n",
      "the cost in 110000 step is 1603.1837788583264\n",
      "the cost in 120000 step is 1600.0887140703244\n",
      "the cost in 130000 step is 1597.3150424818698\n",
      "the cost in 140000 step is 1594.8017887330095\n",
      "the cost in 150000 step is 1592.50270063014\n",
      "the cost in 160000 step is 1590.3822915300527\n",
      "the cost in 170000 step is 1588.4130121438147\n",
      "the cost in 180000 step is 1586.5732242336433\n",
      "the cost in 190000 step is 1584.8457454577533\n",
      "the cost in 200000 step is 1583.2168010445241\n",
      "the cost in 0 step is 43057.13023319075\n",
      "the cost in 10000 step is 743.2105174581436\n",
      "the cost in 20000 step is 705.3247057774153\n",
      "the cost in 30000 step is 685.578184657003\n",
      "the cost in 40000 step is 672.8821467495865\n",
      "the cost in 50000 step is 663.8870778549996\n",
      "the cost in 60000 step is 657.1372684968447\n",
      "the cost in 70000 step is 651.8691983571443\n",
      "the cost in 80000 step is 647.6346427959295\n",
      "the cost in 90000 step is 644.1505519906297\n",
      "the cost in 100000 step is 641.2287442404942\n",
      "the cost in 110000 step is 638.7391307975424\n",
      "the cost in 120000 step is 636.5889362581952\n",
      "the cost in 130000 step is 634.7102634837019\n",
      "the cost in 140000 step is 633.0523115558625\n",
      "the cost in 150000 step is 631.576329325664\n",
      "the cost in 160000 step is 630.2522470128223\n",
      "the cost in 170000 step is 629.0563724529777\n",
      "the cost in 180000 step is 627.9697814851294\n",
      "the cost in 190000 step is 626.9771714816476\n",
      "the cost in 200000 step is 626.0660302950516\n",
      "the cost in 0 step is 40041.1028637697\n",
      "the cost in 10000 step is 870.0987054949602\n",
      "the cost in 20000 step is 820.1844442206137\n",
      "the cost in 30000 step is 795.1644045015648\n",
      "the cost in 40000 step is 779.3791886041912\n",
      "the cost in 50000 step is 768.1676140523836\n",
      "the cost in 60000 step is 759.5843307805343\n",
      "the cost in 70000 step is 752.6713649924151\n",
      "the cost in 80000 step is 746.9021552362489\n",
      "the cost in 90000 step is 741.9618415260306\n",
      "the cost in 100000 step is 737.6489303608413\n",
      "the cost in 110000 step is 733.8271476526054\n",
      "the cost in 120000 step is 730.4000943487407\n",
      "the cost in 130000 step is 727.2970533157587\n",
      "the cost in 140000 step is 724.4645980314056\n",
      "the cost in 150000 step is 721.8613887901209\n",
      "the cost in 160000 step is 719.4548076207383\n",
      "the cost in 170000 step is 717.2187011168194\n",
      "the cost in 180000 step is 715.1318169230318\n",
      "the cost in 190000 step is 713.1766889817376\n",
      "the cost in 200000 step is 711.3388210894836\n",
      "the cost in 0 step is 48774.711780999794\n",
      "the cost in 10000 step is 2594.1305563907304\n",
      "the cost in 20000 step is 2566.009362965211\n",
      "the cost in 30000 step is 2552.9363472600594\n",
      "the cost in 40000 step is 2544.794264635882\n",
      "the cost in 50000 step is 2538.9873528679345\n",
      "the cost in 60000 step is 2534.5351050054383\n",
      "the cost in 70000 step is 2530.9680014922924\n",
      "the cost in 80000 step is 2528.0231322470154\n",
      "the cost in 90000 step is 2525.537651287596\n",
      "the cost in 100000 step is 2523.403751443929\n",
      "the cost in 110000 step is 2521.546623361933\n",
      "the cost in 120000 step is 2519.9124399113703\n",
      "the cost in 130000 step is 2518.4612952410835\n",
      "the cost in 140000 step is 2517.1628333398785\n",
      "the cost in 150000 step is 2515.993437308706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cost in 160000 step is 2514.934367728348\n",
      "the cost in 170000 step is 2513.9704981096406\n",
      "the cost in 180000 step is 2513.0894359943563\n",
      "the cost in 190000 step is 2512.280898729597\n",
      "the cost in 200000 step is 2511.536260830699\n",
      "the cost in 0 step is 44391.80966426329\n",
      "the cost in 10000 step is 2247.5034380243783\n",
      "the cost in 20000 step is 2212.715214264926\n",
      "the cost in 30000 step is 2195.4240583827955\n",
      "the cost in 40000 step is 2184.1982696731015\n",
      "the cost in 50000 step is 2176.0377929010806\n",
      "the cost in 60000 step is 2169.733048193807\n",
      "the cost in 70000 step is 2164.6769022977046\n",
      "the cost in 80000 step is 2160.5177222489415\n",
      "the cost in 90000 step is 2157.030070925836\n",
      "the cost in 100000 step is 2154.059335548022\n",
      "the cost in 110000 step is 2151.4948776040324\n",
      "the cost in 120000 step is 2149.255236758325\n",
      "the cost in 130000 step is 2147.2791309216004\n",
      "the cost in 140000 step is 2145.519634292278\n",
      "the cost in 150000 step is 2143.940262389375\n",
      "the cost in 160000 step is 2142.512265093066\n",
      "the cost in 170000 step is 2141.2127127018366\n",
      "the cost in 180000 step is 2140.0231169173326\n",
      "the cost in 190000 step is 2138.928420883862\n",
      "the cost in 200000 step is 2137.9162488085763\n",
      "Iteration took 74444.132998 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# calculate weight seperately\n",
    "learning_rate = 0.00002\n",
    "iteration_time = 200001\n",
    "tic = time.time()\n",
    "w0 = JS.gradient_descent1(y0_train, learning_rate, iteration_time)\n",
    "w1 = JS.gradient_descent1(y1_train, learning_rate, iteration_time)\n",
    "w2 = JS.gradient_descent1(y2_train, learning_rate, iteration_time)\n",
    "w3 = JS.gradient_descent1(y3_train, learning_rate, iteration_time)\n",
    "w4 = JS.gradient_descent1(y4_train, learning_rate, iteration_time)\n",
    "w5 = JS.gradient_descent1(y5_train, learning_rate, iteration_time)\n",
    "w6 = JS.gradient_descent1(y6_train, learning_rate, iteration_time)\n",
    "w7 = JS.gradient_descent1(y7_train, learning_rate, iteration_time)\n",
    "w8 = JS.gradient_descent1(y8_train, learning_rate, iteration_time)\n",
    "w9 = JS.gradient_descent1(y9_train, learning_rate, iteration_time)\n",
    "toc = time.time()\n",
    "print('Iteration took %f seconds' %(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhaXJwxRsm9_"
   },
   "outputs": [],
   "source": [
    "# calculate probability for each category\n",
    "y_pred = []\n",
    "for i in range(np.shape(x_test)[0]):\n",
    "\n",
    "    pred_0 = JS.get_pcx(x_test[i], w0)\n",
    "    pred_1 = JS.get_pcx(x_test[i], w1)\n",
    "    pred_2 = JS.get_pcx(x_test[i], w2)\n",
    "    pred_3 = JS.get_pcx(x_test[i], w3)\n",
    "    pred_4 = JS.get_pcx(x_test[i], w4)\n",
    "    pred_5 = JS.get_pcx(x_test[i], w5)\n",
    "    pred_6 = JS.get_pcx(x_test[i], w6)\n",
    "    pred_7 = JS.get_pcx(x_test[i], w7)\n",
    "    pred_8 = JS.get_pcx(x_test[i], w8)\n",
    "    pred_9 = JS.get_pcx(x_test[i], w9)\n",
    "    pred = [pred_0[0],pred_1[0],pred_2[0],pred_3[0],pred_4[0],\n",
    "            pred_5[0],pred_6[0],pred_7[0],pred_8[0],pred_9[0]]\n",
    "    value = JS.predict_c(pred)\n",
    "    y_pred.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IG7DfAybsnAa",
    "outputId": "7746a08e-1a46-4b7a-8f42-fb72bcc8f5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9103 / 10000 correct\n",
      "Accuracy = 0.910300\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Jtb5uGhsnC0",
    "outputId": "8be8318f-48fa-4fc8-b446-f69fe2eb360f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 948    0    4    3    0   11    8    2    2    2]\n",
      " [   0 1109    3    2    1    1    4    1   13    1]\n",
      " [   7   12  909   28    9    4   10   12   38    3]\n",
      " [   4    3   20  913    4   20    3   13   22    8]\n",
      " [   1    1    5    6  910    0   10    4    9   36]\n",
      " [  13    4    7   36    8  765   11    6   36    6]\n",
      " [  10    3   13    0    4   22  902    1    3    0]\n",
      " [   4    8   20    4    7    4    1  932    6   42]\n",
      " [   9   11   10   23   14   28   15   15  836   13]\n",
      " [   8    9    1   18   37   14    0   30   13  879]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.967     0.956       980\n",
      "           1      0.956     0.977     0.966      1135\n",
      "           2      0.916     0.881     0.898      1032\n",
      "           3      0.884     0.904     0.894      1010\n",
      "           4      0.915     0.927     0.921       982\n",
      "           5      0.880     0.858     0.869       892\n",
      "           6      0.936     0.942     0.939       958\n",
      "           7      0.917     0.907     0.912      1028\n",
      "           8      0.855     0.858     0.857       974\n",
      "           9      0.888     0.871     0.879      1009\n",
      "\n",
      "   micro avg      0.910     0.910     0.910     10000\n",
      "   macro avg      0.909     0.909     0.909     10000\n",
      "weighted avg      0.910     0.910     0.910     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVvxu_PlsnFT"
   },
   "outputs": [],
   "source": [
    "# PCA + Logistic Regression\n",
    "## PCA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCNHhlMysnHq"
   },
   "outputs": [],
   "source": [
    "# download MNIST dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# convert data type to float 32\n",
    "x_train=x_train\n",
    "y_train=y_train\n",
    "x_train=np.float32(x_train)\n",
    "x_test=np.float32(x_test)\n",
    "x_train = x_train.reshape(np.shape(x_train)[0], 28*28)/255.0\n",
    "x_test = x_test.reshape(np.shape(x_test)[0], 28*28)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGS6wvK_eIxw"
   },
   "outputs": [],
   "source": [
    "# stack them as a big one for dimension deduction\n",
    "big_X=np.vstack((x_train,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGjkEmansnKb"
   },
   "outputs": [],
   "source": [
    "# build PCA class\n",
    "class SJPCA(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X):\n",
    "        self.x_train = X\n",
    "    \n",
    "    def compute_mean_covar_eigen(self):\n",
    "        # get average image and get mean image by summing each row\n",
    "        tr_mean = np.mean(self.x_train, axis=0)\n",
    "        tr_mean = np.reshape(tr_mean,(1,np.shape(tr_mean)[0]))\n",
    "\n",
    "        # subtract the mean\n",
    "        xtr_m = self.x_train - tr_mean\n",
    "\n",
    "        # calculate covariance matrix\n",
    "        tr_cov = np.dot(xtr_m.T,xtr_m)\n",
    "        \n",
    "        # get eigenvalue and eigenvector\n",
    "        tr_val, tr_vec = eig(tr_cov)\n",
    "        \n",
    "        return xtr_m, tr_cov, tr_val, tr_vec\n",
    "    \n",
    "    def get_comp_K(self,tr_val, threshold):\n",
    "        cum_lambda = np.cumsum(tr_val)\n",
    "        total_lamda = cum_lambda[-1]\n",
    "        \n",
    "        # get the principal component number that we want to keep\n",
    "        for keep_dim in range(len(tr_val)):\n",
    "            rate = cum_lambda[keep_dim]/total_lamda\n",
    "            if rate >= threshold: \n",
    "                return keep_dim\n",
    "                break\n",
    "            else: continue\n",
    "            \n",
    "    def deduct_img(self, tr_vec, keep_dim):\n",
    "        x_proj= np.dot(self.x_train, tr_vec.T[:,0:keep_dim])\n",
    "        return x_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75_zL14ZeIx1"
   },
   "source": [
    "## Retain 90% Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GOOr1mTHeIx1",
    "outputId": "f26d2499-2811-44f3-8245-ed405b71f6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "# Deduct Training Set\n",
    "SJ = SJPCA()\n",
    "SJ.train(big_X)\n",
    "xtr_m, tr_cov, tr_val, tr_vec = SJ.compute_mean_covar_eigen()\n",
    "keep_dim = SJ.get_comp_K(tr_val, 0.90)\n",
    "new_big_X = SJ.deduct_img(tr_vec, keep_dim)\n",
    "print(keep_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qokZ3EveIx4"
   },
   "outputs": [],
   "source": [
    "# resplit the dataset and normalize them with min-max normalization\n",
    "x_train = new_big_X[0:60000,:]\n",
    "x_test = new_big_X[60000:70000,:]\n",
    "tr_min = np.min(x_train,axis=1)\n",
    "tr_cha = np.max(x_train,axis=1)-np.min(x_train,axis=1)\n",
    "te_min = np.min(x_test,axis=1)\n",
    "te_cha = np.max(x_test,axis=1)-np.min(x_test,axis=1)\n",
    "for i in range(60000):\n",
    "    x_train[i]=(x_train[i]-tr_min[i])/tr_cha[i]\n",
    "for i in range(10000):\n",
    "    x_test[i]=(x_test[i]-te_min[i])/te_cha[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBPRG7qreIx5"
   },
   "outputs": [],
   "source": [
    "# split the train as 10 categories\n",
    "x_train = x_train[0:25000,]\n",
    "y_train = y_train[0:25000,]\n",
    "JS = SJLogis_Regre()\n",
    "JS.train(x_train,y_train)\n",
    "y0_train = JS.split_category1(0)\n",
    "y1_train = JS.split_category1(1)\n",
    "y2_train = JS.split_category1(2)\n",
    "y3_train = JS.split_category1(3)\n",
    "y4_train = JS.split_category1(4)\n",
    "y5_train = JS.split_category1(5)\n",
    "y6_train = JS.split_category1(6)\n",
    "y7_train = JS.split_category1(7)\n",
    "y8_train = JS.split_category1(8)\n",
    "y9_train = JS.split_category1(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3604
    },
    "colab_type": "code",
    "id": "a_FKWV0_eIx7",
    "outputId": "194f27e8-d699-4e8d-9b58-0047f248603a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cost in 0 step is 11494.348784778926\n",
      "the cost in 10000 step is 3126.412584415647\n",
      "the cost in 20000 step is 3019.222058121433\n",
      "the cost in 30000 step is 2980.548900777659\n",
      "the cost in 40000 step is 2960.0637610517406\n",
      "the cost in 50000 step is 2947.133525428829\n",
      "the cost in 60000 step is 2938.150683857481\n",
      "the cost in 70000 step is 2931.5252993140994\n",
      "the cost in 80000 step is 2926.4329330647543\n",
      "the cost in 90000 step is 2922.3987258713228\n",
      "the cost in 100000 step is 2919.12874921704\n",
      "the cost in 110000 step is 2916.4309997558444\n",
      "the cost in 120000 step is 2914.1744244347587\n",
      "the cost in 130000 step is 2912.266118229002\n",
      "the cost in 140000 step is 2910.637984798607\n",
      "the cost in 150000 step is 2909.238631302235\n",
      "the cost in 160000 step is 2908.0282850389312\n",
      "the cost in 170000 step is 2906.9755114693517\n",
      "the cost in 180000 step is 2906.0550344696912\n",
      "the cost in 190000 step is 2905.24624699049\n",
      "the cost in 200000 step is 2904.5321641235228\n",
      "the cost in 0 step is 13195.457526007835\n",
      "the cost in 10000 step is 2605.1631217709282\n",
      "the cost in 20000 step is 2527.082562647482\n",
      "the cost in 30000 step is 2499.6754922581245\n",
      "the cost in 40000 step is 2485.788203774236\n",
      "the cost in 50000 step is 2477.2256707642114\n",
      "the cost in 60000 step is 2471.252817453685\n",
      "the cost in 70000 step is 2466.730449477056\n",
      "the cost in 80000 step is 2463.109379705213\n",
      "the cost in 90000 step is 2460.0948624613125\n",
      "the cost in 100000 step is 2457.514770864773\n",
      "the cost in 110000 step is 2455.26136017355\n",
      "the cost in 120000 step is 2453.2631403257556\n",
      "the cost in 130000 step is 2451.470289348657\n",
      "the cost in 140000 step is 2449.8466256742327\n",
      "the cost in 150000 step is 2448.3649560534345\n",
      "the cost in 160000 step is 2447.004252767274\n",
      "the cost in 170000 step is 2445.7478683169693\n",
      "the cost in 180000 step is 2444.5823631870967\n",
      "the cost in 190000 step is 2443.496709734964\n",
      "the cost in 200000 step is 2442.4817349064124\n",
      "the cost in 0 step is 11993.164788890155\n",
      "the cost in 10000 step is 4253.653600759702\n",
      "the cost in 20000 step is 4185.437801964854\n",
      "the cost in 30000 step is 4163.6179129090915\n",
      "the cost in 40000 step is 4153.833177792004\n",
      "the cost in 50000 step is 4148.465262167557\n",
      "the cost in 60000 step is 4144.997092878471\n",
      "the cost in 70000 step is 4142.453628649349\n",
      "the cost in 80000 step is 4140.418135989224\n",
      "the cost in 90000 step is 4138.696054782977\n",
      "the cost in 100000 step is 4137.187631041936\n",
      "the cost in 110000 step is 4135.836447054798\n",
      "the cost in 120000 step is 4134.607464676719\n",
      "the cost in 130000 step is 4133.477112887989\n",
      "the cost in 140000 step is 4132.428514731237\n",
      "the cost in 150000 step is 4131.449016214509\n",
      "the cost in 160000 step is 4130.528805732905\n",
      "the cost in 170000 step is 4129.660084220133\n",
      "the cost in 180000 step is 4128.836533230995\n",
      "the cost in 190000 step is 4128.052955577392\n",
      "the cost in 200000 step is 4127.30502232009\n",
      "the cost in 0 step is 11774.667289332769\n",
      "the cost in 10000 step is 4750.796715164135\n",
      "the cost in 20000 step is 4713.998764948993\n",
      "the cost in 30000 step is 4698.930887031426\n",
      "the cost in 40000 step is 4688.53318221257\n",
      "the cost in 50000 step is 4680.67877827822\n",
      "the cost in 60000 step is 4674.551664926897\n",
      "the cost in 70000 step is 4669.680736350771\n",
      "the cost in 80000 step is 4665.754818964564\n",
      "the cost in 90000 step is 4662.55463788173\n",
      "the cost in 100000 step is 4659.919566124383\n",
      "the cost in 110000 step is 4657.728939952542\n",
      "the cost in 120000 step is 4655.890558134034\n",
      "the cost in 130000 step is 4654.333116809073\n",
      "the cost in 140000 step is 4653.0009837541165\n",
      "the cost in 150000 step is 4651.850457853401\n",
      "the cost in 160000 step is 4650.847021825205\n",
      "the cost in 170000 step is 4649.963286298036\n",
      "the cost in 180000 step is 4649.177429841478\n",
      "the cost in 190000 step is 4648.472002930839\n",
      "the cost in 200000 step is 4647.833003566164\n",
      "the cost in 0 step is 11174.437518526476\n",
      "the cost in 10000 step is 3798.2646136405097\n",
      "the cost in 20000 step is 3705.0244009201565\n",
      "the cost in 30000 step is 3655.5708216293774\n",
      "the cost in 40000 step is 3622.8815114186787\n",
      "the cost in 50000 step is 3599.319775265019\n",
      "the cost in 60000 step is 3581.4634712570905\n",
      "the cost in 70000 step is 3567.4562005125076\n",
      "the cost in 80000 step is 3556.180880429334\n",
      "the cost in 90000 step is 3546.9189532149076\n",
      "the cost in 100000 step is 3539.185338874341\n",
      "the cost in 110000 step is 3532.6398782979672\n",
      "the cost in 120000 step is 3527.0363876786523\n",
      "the cost in 130000 step is 3522.191837814985\n",
      "the cost in 140000 step is 3517.9669701581934\n",
      "the cost in 150000 step is 3514.2537077720235\n",
      "the cost in 160000 step is 3510.9667452725607\n",
      "the cost in 170000 step is 3508.037783331035\n",
      "the cost in 180000 step is 3505.4114792046084\n",
      "the cost in 190000 step is 3503.042536846499\n",
      "the cost in 200000 step is 3500.893570687847\n",
      "the cost in 0 step is 10716.23967403196\n",
      "the cost in 10000 step is 5436.415826873967\n",
      "the cost in 20000 step is 5414.813068150808\n",
      "the cost in 30000 step is 5406.710458184604\n",
      "the cost in 40000 step is 5402.006825231792\n",
      "the cost in 50000 step is 5398.800746295721\n",
      "the cost in 60000 step is 5396.413353565801\n",
      "the cost in 70000 step is 5394.531698486926\n",
      "the cost in 80000 step is 5392.987838659084\n",
      "the cost in 90000 step is 5391.682122950585\n",
      "the cost in 100000 step is 5390.551079925108\n",
      "the cost in 110000 step is 5389.552123917892\n",
      "the cost in 120000 step is 5388.655542301073\n",
      "the cost in 130000 step is 5387.839971913611\n",
      "the cost in 140000 step is 5387.089685669847\n",
      "the cost in 150000 step is 5386.392881964512\n",
      "the cost in 160000 step is 5385.740561547407\n",
      "the cost in 170000 step is 5385.1257654675055\n",
      "the cost in 180000 step is 5384.543044173435\n",
      "the cost in 190000 step is 5383.988079737385\n",
      "the cost in 200000 step is 5383.457412405318\n",
      "the cost in 0 step is 12859.308589492144\n",
      "the cost in 10000 step is 3005.5981288791972\n",
      "the cost in 20000 step is 2956.692565328443\n",
      "the cost in 30000 step is 2943.685460772231\n",
      "the cost in 40000 step is 2937.0087742005244\n",
      "the cost in 50000 step is 2932.3870519743114\n",
      "the cost in 60000 step is 2928.760658782249\n",
      "the cost in 70000 step is 2925.738812734335\n",
      "the cost in 80000 step is 2923.12993209388\n",
      "the cost in 90000 step is 2920.8224317954337\n",
      "the cost in 100000 step is 2918.7445519804164\n",
      "the cost in 110000 step is 2916.847236335628\n",
      "the cost in 120000 step is 2915.095503525564\n",
      "the cost in 130000 step is 2913.4635959083494\n",
      "the cost in 140000 step is 2911.9320490031478\n",
      "the cost in 150000 step is 2910.485830055776\n",
      "the cost in 160000 step is 2909.1131098477463\n",
      "the cost in 170000 step is 2907.8044270487117\n",
      "the cost in 180000 step is 2906.5521047408456\n",
      "the cost in 190000 step is 2905.349833795178\n",
      "the cost in 200000 step is 2904.19236948099\n",
      "the cost in 0 step is 11515.196386206519\n",
      "the cost in 10000 step is 3927.0088302984695\n",
      "the cost in 20000 step is 3838.3086231997477\n",
      "the cost in 30000 step is 3801.4069304543887\n",
      "the cost in 40000 step is 3779.6211128270634\n",
      "the cost in 50000 step is 3764.541929526753\n",
      "the cost in 60000 step is 3753.120653481457\n",
      "the cost in 70000 step is 3743.9754571195313\n",
      "the cost in 80000 step is 3736.3835091469837\n",
      "the cost in 90000 step is 3729.9232784664614\n",
      "the cost in 100000 step is 3724.3271330031675\n",
      "the cost in 110000 step is 3719.413658292306\n",
      "the cost in 120000 step is 3715.0535652833064\n",
      "the cost in 130000 step is 3711.151066742504\n",
      "the cost in 140000 step is 3707.6329812625895\n",
      "the cost in 150000 step is 3704.4419896256645\n",
      "the cost in 160000 step is 3701.5322661376663\n",
      "the cost in 170000 step is 3698.8665405674983\n",
      "the cost in 180000 step is 3696.4140592813555\n",
      "the cost in 190000 step is 3694.149131826925\n",
      "the cost in 200000 step is 3692.0500702217582\n",
      "the cost in 0 step is 11479.217210392377\n",
      "the cost in 10000 step is 5342.85144675758\n",
      "the cost in 20000 step is 5284.176358784428\n",
      "the cost in 30000 step is 5262.125851555122\n",
      "the cost in 40000 step is 5250.825193455512\n",
      "the cost in 50000 step is 5243.926832598075\n",
      "the cost in 60000 step is 5239.112025853081\n",
      "the cost in 70000 step is 5235.411207281902\n",
      "the cost in 80000 step is 5232.375465471756\n",
      "the cost in 90000 step is 5229.7764376520345\n",
      "the cost in 100000 step is 5227.487120984874\n",
      "the cost in 110000 step is 5225.430862671986\n",
      "the cost in 120000 step is 5223.557975287069\n",
      "the cost in 130000 step is 5221.834276587008\n",
      "the cost in 140000 step is 5220.235093559294\n",
      "the cost in 150000 step is 5218.741921589191\n",
      "the cost in 160000 step is 5217.340450753148\n",
      "the cost in 170000 step is 5216.0193369679455\n",
      "the cost in 180000 step is 5214.769401307368\n",
      "the cost in 190000 step is 5213.583087844729\n",
      "the cost in 200000 step is 5212.454084550456\n",
      "the cost in 0 step is 11023.377253398028\n",
      "the cost in 10000 step is 4876.590964464908\n",
      "the cost in 20000 step is 4856.594228990089\n",
      "the cost in 30000 step is 4847.622860851587\n",
      "the cost in 40000 step is 4841.871770455712\n",
      "the cost in 50000 step is 4837.698306952079\n",
      "the cost in 60000 step is 4834.46388330204\n",
      "the cost in 70000 step is 4831.84183116465\n",
      "the cost in 80000 step is 4829.640220184657\n",
      "the cost in 90000 step is 4827.737962687289\n",
      "the cost in 100000 step is 4826.055439231063\n",
      "the cost in 110000 step is 4824.538798048574\n",
      "the cost in 120000 step is 4823.15080248661\n",
      "the cost in 130000 step is 4821.865211698995\n",
      "the cost in 140000 step is 4820.6632102301965\n",
      "the cost in 150000 step is 4819.531081336323\n",
      "the cost in 160000 step is 4818.458659884766\n",
      "the cost in 170000 step is 4817.438286108525\n",
      "the cost in 180000 step is 4816.464087732435\n",
      "the cost in 190000 step is 4815.531481129755\n",
      "the cost in 200000 step is 4814.636820770846\n",
      "Iteration took 8354.994221 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# calculate weight seperately\n",
    "learning_rate = 0.00002\n",
    "iteration_time = 200001\n",
    "tic = time.time()\n",
    "w0 = JS.gradient_descent1(y0_train, learning_rate, iteration_time)\n",
    "w1 = JS.gradient_descent1(y1_train, learning_rate, iteration_time)\n",
    "w2 = JS.gradient_descent1(y2_train, learning_rate, iteration_time)\n",
    "w3 = JS.gradient_descent1(y3_train, learning_rate, iteration_time)\n",
    "w4 = JS.gradient_descent1(y4_train, learning_rate, iteration_time)\n",
    "w5 = JS.gradient_descent1(y5_train, learning_rate, iteration_time)\n",
    "w6 = JS.gradient_descent1(y6_train, learning_rate, iteration_time)\n",
    "w7 = JS.gradient_descent1(y7_train, learning_rate, iteration_time)\n",
    "w8 = JS.gradient_descent1(y8_train, learning_rate, iteration_time)\n",
    "w9 = JS.gradient_descent1(y9_train, learning_rate, iteration_time)\n",
    "toc = time.time()\n",
    "print('Iteration took %f seconds' %(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_7Rd2OVeIx9"
   },
   "outputs": [],
   "source": [
    "# calculate probability for each category\n",
    "y_pred = []\n",
    "for i in range(np.shape(x_test)[0]):\n",
    "\n",
    "    pred_0 = JS.get_pcx(x_test[i], w0)\n",
    "    pred_1 = JS.get_pcx(x_test[i], w1)\n",
    "    pred_2 = JS.get_pcx(x_test[i], w2)\n",
    "    pred_3 = JS.get_pcx(x_test[i], w3)\n",
    "    pred_4 = JS.get_pcx(x_test[i], w4)\n",
    "    pred_5 = JS.get_pcx(x_test[i], w5)\n",
    "    pred_6 = JS.get_pcx(x_test[i], w6)\n",
    "    pred_7 = JS.get_pcx(x_test[i], w7)\n",
    "    pred_8 = JS.get_pcx(x_test[i], w8)\n",
    "    pred_9 = JS.get_pcx(x_test[i], w9)\n",
    "    pred = [pred_0[0],pred_1[0],pred_2[0],pred_3[0],pred_4[0],\n",
    "            pred_5[0],pred_6[0],pred_7[0],pred_8[0],pred_9[0]]\n",
    "    value = JS.predict_c(pred)\n",
    "    y_pred.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EmCP3e2geIyA",
    "outputId": "35884d79-1eaf-4b02-9c32-eea16d44999c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7197 / 10000 correct\n",
      "Accuracy = 0.719700\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "hCyxYF4peIyD",
    "outputId": "1a6b2ef6-a27f-478b-f0f9-4ddea9aa33e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 819    0   14   37    4   28   51    2   22    3]\n",
      " [   1 1037   21   11   14    4    7    1   38    1]\n",
      " [  34   55  685   45   24   28   84    3   67    7]\n",
      " [  40   13   61  731   17   49   28   17   35   19]\n",
      " [   6   41   31   20  745   15   19   13   19   73]\n",
      " [  38   30   34  199   52  375   44    9   61   50]\n",
      " [  42   10   50    5   33    9  784    5   19    1]\n",
      " [   3   34   10   14   41    9    8  784   17  108]\n",
      " [  92   43   27   43   37   58   31   11  613   19]\n",
      " [  10    8    9   27   75   16    2  210   28  624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.755     0.836     0.793       980\n",
      "           1      0.816     0.914     0.862      1135\n",
      "           2      0.727     0.664     0.694      1032\n",
      "           3      0.646     0.724     0.683      1010\n",
      "           4      0.715     0.759     0.736       982\n",
      "           5      0.635     0.420     0.506       892\n",
      "           6      0.741     0.818     0.778       958\n",
      "           7      0.743     0.763     0.753      1028\n",
      "           8      0.667     0.629     0.648       974\n",
      "           9      0.690     0.618     0.652      1009\n",
      "\n",
      "   micro avg      0.720     0.720     0.720     10000\n",
      "   macro avg      0.713     0.714     0.710     10000\n",
      "weighted avg      0.716     0.720     0.714     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZdoNz_ReIyF"
   },
   "source": [
    "## Retain 95% Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuLxkYaeeIyG",
    "outputId": "6cc84e7c-333f-411f-937c-0eebbe3aea3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "# Deduct Training Set\n",
    "SJ = SJPCA()\n",
    "SJ.train(big_X)\n",
    "xtr_m, tr_cov, tr_val, tr_vec = SJ.compute_mean_covar_eigen()\n",
    "keep_dim = SJ.get_comp_K(tr_val, 0.95)\n",
    "new_big_X = SJ.deduct_img(tr_vec, keep_dim)\n",
    "print(keep_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QrLlLEneIyI"
   },
   "outputs": [],
   "source": [
    "# resplit the dataset and normalize them with min-max normalization\n",
    "x_train = new_big_X[0:60000,:]\n",
    "x_test = new_big_X[60000:70000,:]\n",
    "tr_min = np.min(x_train,axis=1)\n",
    "tr_cha = np.max(x_train,axis=1)-np.min(x_train,axis=1)\n",
    "te_min = np.min(x_test,axis=1)\n",
    "te_cha = np.max(x_test,axis=1)-np.min(x_test,axis=1)\n",
    "for i in range(60000):\n",
    "    x_train[i]=(x_train[i]-tr_min[i])/tr_cha[i]\n",
    "for i in range(10000):\n",
    "    x_test[i]=(x_test[i]-te_min[i])/te_cha[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uq8DuvMmeIyL"
   },
   "outputs": [],
   "source": [
    "# split the train as 10 categories\n",
    "x_train = x_train[0:25000,]\n",
    "y_train = y_train[0:25000,]\n",
    "JS = SJLogis_Regre()\n",
    "JS.train(x_train,y_train)\n",
    "y0_train = JS.split_category1(0)\n",
    "y1_train = JS.split_category1(1)\n",
    "y2_train = JS.split_category1(2)\n",
    "y3_train = JS.split_category1(3)\n",
    "y4_train = JS.split_category1(4)\n",
    "y5_train = JS.split_category1(5)\n",
    "y6_train = JS.split_category1(6)\n",
    "y7_train = JS.split_category1(7)\n",
    "y8_train = JS.split_category1(8)\n",
    "y9_train = JS.split_category1(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvrLcidUeIyN",
    "outputId": "38a7c738-d27e-42e8-b05c-7de6603832ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cost in 0 step is 21481.39931363333\n",
      "the cost in 10000 step is 1290.979156247232\n",
      "the cost in 20000 step is 1179.9316720640436\n",
      "the cost in 30000 step is 1136.8498801548822\n",
      "the cost in 40000 step is 1113.610628108722\n",
      "the cost in 50000 step is 1098.8877151910638\n",
      "the cost in 60000 step is 1088.626968789076\n",
      "the cost in 70000 step is 1081.0106065444509\n",
      "the cost in 80000 step is 1075.0973584090684\n",
      "the cost in 90000 step is 1070.3489266354475\n",
      "the cost in 100000 step is 1066.434427246745\n",
      "the cost in 110000 step is 1063.1389819309168\n",
      "the cost in 120000 step is 1060.3168282585434\n",
      "the cost in 130000 step is 1057.8654896608593\n",
      "the cost in 140000 step is 1055.7107270994716\n",
      "the cost in 150000 step is 1053.7973652048368\n",
      "the cost in 160000 step is 1052.0834835329895\n",
      "the cost in 170000 step is 1050.5366169529138\n",
      "the cost in 180000 step is 1049.1311985537595\n",
      "the cost in 190000 step is 1047.846794906433\n",
      "the cost in 200000 step is 1046.6668605452846\n",
      "the cost in 0 step is 24718.107437320756\n",
      "the cost in 10000 step is 1151.1126723662248\n",
      "the cost in 20000 step is 1066.7136659425819\n",
      "the cost in 30000 step is 1027.6167318068062\n",
      "the cost in 40000 step is 1003.9998753464371\n",
      "the cost in 50000 step is 988.0094353940834\n",
      "the cost in 60000 step is 976.4594199220651\n",
      "the cost in 70000 step is 967.7520814664608\n",
      "the cost in 80000 step is 960.979598913985\n",
      "the cost in 90000 step is 955.5820962287655\n",
      "the cost in 100000 step is 951.1937271701398\n",
      "the cost in 110000 step is 947.5649766267343\n",
      "the cost in 120000 step is 944.5199778282936\n",
      "the cost in 130000 step is 941.9314899674977\n",
      "the cost in 140000 step is 939.7054594937763\n",
      "the cost in 150000 step is 937.7710981774508\n",
      "the cost in 160000 step is 936.0742930364772\n",
      "the cost in 170000 step is 934.5731091289713\n",
      "the cost in 180000 step is 933.234650187493\n",
      "the cost in 190000 step is 932.0328243210581\n",
      "the cost in 200000 step is 930.9467269463917\n",
      "the cost in 0 step is 21218.52266257611\n",
      "the cost in 10000 step is 2608.734637799854\n",
      "the cost in 20000 step is 2486.454143185559\n",
      "the cost in 30000 step is 2437.877724271532\n",
      "the cost in 40000 step is 2411.8854477581235\n",
      "the cost in 50000 step is 2396.0875464441506\n",
      "the cost in 60000 step is 2385.7768270080437\n",
      "the cost in 70000 step is 2378.7126633745856\n",
      "the cost in 80000 step is 2373.685525144285\n",
      "the cost in 90000 step is 2369.990643722943\n",
      "the cost in 100000 step is 2367.1958621303056\n",
      "the cost in 110000 step is 2365.026052464628\n",
      "the cost in 120000 step is 2363.3007313845874\n",
      "the cost in 130000 step is 2361.898455859137\n",
      "the cost in 140000 step is 2360.735657288192\n",
      "the cost in 150000 step is 2359.7536535561653\n",
      "the cost in 160000 step is 2358.9104681652975\n",
      "the cost in 170000 step is 2358.1755592719614\n",
      "the cost in 180000 step is 2357.526353823065\n",
      "the cost in 190000 step is 2356.9459255534844\n",
      "the cost in 200000 step is 2356.4214118910286\n",
      "the cost in 0 step is 21446.709402156328\n",
      "the cost in 10000 step is 2850.00157956573\n",
      "the cost in 20000 step is 2772.696637781348\n",
      "the cost in 30000 step is 2744.191092728533\n",
      "the cost in 40000 step is 2729.400104717881\n",
      "the cost in 50000 step is 2720.184920198146\n",
      "the cost in 60000 step is 2713.767087775794\n",
      "the cost in 70000 step is 2708.9654594612743\n",
      "the cost in 80000 step is 2705.1960044236616\n",
      "the cost in 90000 step is 2702.134881409138\n",
      "the cost in 100000 step is 2699.585785238403\n",
      "the cost in 110000 step is 2697.4212559585594\n",
      "the cost in 120000 step is 2695.554086582456\n",
      "the cost in 130000 step is 2693.9221847954063\n",
      "the cost in 140000 step is 2692.4799592895497\n",
      "the cost in 150000 step is 2691.193110456793\n",
      "the cost in 160000 step is 2690.035317429438\n",
      "the cost in 170000 step is 2688.986043480984\n",
      "the cost in 180000 step is 2688.0290336786334\n",
      "the cost in 190000 step is 2687.1512586001163\n",
      "the cost in 200000 step is 2686.342155138763\n",
      "the cost in 0 step is 20795.722369229996\n",
      "the cost in 10000 step is 2070.384883555123\n",
      "the cost in 20000 step is 1963.9199870251264\n",
      "the cost in 30000 step is 1922.5514990173167\n",
      "the cost in 40000 step is 1900.5117452659697\n",
      "the cost in 50000 step is 1886.7223966820982\n",
      "the cost in 60000 step is 1877.2139147898383\n",
      "the cost in 70000 step is 1870.2263514889162\n",
      "the cost in 80000 step is 1864.861562391915\n",
      "the cost in 90000 step is 1860.6114038332978\n",
      "the cost in 100000 step is 1857.164506701705\n",
      "the cost in 110000 step is 1854.3176741517289\n",
      "the cost in 120000 step is 1851.9315195784166\n",
      "the cost in 130000 step is 1849.9065849012675\n",
      "the cost in 140000 step is 1848.1696613729766\n",
      "the cost in 150000 step is 1846.6655194813204\n",
      "the cost in 160000 step is 1845.3516677710577\n",
      "the cost in 170000 step is 1844.194892776376\n",
      "the cost in 180000 step is 1843.1688933879636\n",
      "the cost in 190000 step is 1842.2526147075337\n",
      "the cost in 200000 step is 1841.4290448339698\n",
      "the cost in 0 step is 19317.28222289082\n",
      "the cost in 10000 step is 3575.185382029389\n",
      "the cost in 20000 step is 3460.5829954966475\n",
      "the cost in 30000 step is 3417.689616286633\n",
      "the cost in 40000 step is 3396.6358994390694\n",
      "the cost in 50000 step is 3384.7145562205437\n",
      "the cost in 60000 step is 3377.190245329684\n",
      "the cost in 70000 step is 3371.9941216118696\n",
      "the cost in 80000 step is 3368.130168217673\n",
      "the cost in 90000 step is 3365.0831494260724\n",
      "the cost in 100000 step is 3362.570036685225\n",
      "the cost in 110000 step is 3360.426715910553\n",
      "the cost in 120000 step is 3358.5530994346514\n",
      "the cost in 130000 step is 3356.8851793819013\n",
      "the cost in 140000 step is 3355.38016591122\n",
      "the cost in 150000 step is 3354.008260526704\n",
      "the cost in 160000 step is 3352.747925925376\n",
      "the cost in 170000 step is 3351.583066013601\n",
      "the cost in 180000 step is 3350.50128616405\n",
      "the cost in 190000 step is 3349.4927851182792\n",
      "the cost in 200000 step is 3348.549628239625\n",
      "the cost in 0 step is 20901.197291827262\n",
      "the cost in 10000 step is 1519.5377431493357\n",
      "the cost in 20000 step is 1444.7516444278635\n",
      "the cost in 30000 step is 1414.820489172695\n",
      "the cost in 40000 step is 1398.7687626530499\n",
      "the cost in 50000 step is 1388.8322942594725\n",
      "the cost in 60000 step is 1382.086372297032\n",
      "the cost in 70000 step is 1377.1955498954237\n",
      "the cost in 80000 step is 1373.4714706326677\n",
      "the cost in 90000 step is 1370.5268377871892\n",
      "the cost in 100000 step is 1368.128402444802\n",
      "the cost in 110000 step is 1366.127763215047\n",
      "the cost in 120000 step is 1364.4261340264209\n",
      "the cost in 130000 step is 1362.955230591273\n",
      "the cost in 140000 step is 1361.6663340576897\n",
      "the cost in 150000 step is 1360.5237407143043\n",
      "the cost in 160000 step is 1359.5006794835951\n",
      "the cost in 170000 step is 1358.576677253663\n",
      "the cost in 180000 step is 1357.7358057669546\n",
      "the cost in 190000 step is 1356.9654833531415\n",
      "the cost in 200000 step is 1356.2556363871279\n",
      "the cost in 0 step is 22035.56148843189\n",
      "the cost in 10000 step is 1977.5659705585051\n",
      "the cost in 20000 step is 1889.0166862059818\n",
      "the cost in 30000 step is 1853.2871074041734\n",
      "the cost in 40000 step is 1833.672836196149\n",
      "the cost in 50000 step is 1821.1783931912528\n",
      "the cost in 60000 step is 1812.4340081164917\n",
      "the cost in 70000 step is 1805.896393868115\n",
      "the cost in 80000 step is 1800.765827619451\n",
      "the cost in 90000 step is 1796.5900928049737\n",
      "the cost in 100000 step is 1793.0956220990286\n",
      "the cost in 110000 step is 1790.1075074530365\n",
      "the cost in 120000 step is 1787.5085474568584\n",
      "the cost in 130000 step is 1785.216968812816\n",
      "the cost in 140000 step is 1783.1736862886985\n",
      "the cost in 150000 step is 1781.334698726024\n",
      "the cost in 160000 step is 1779.6663757001656\n",
      "the cost in 170000 step is 1778.1424347969544\n",
      "the cost in 180000 step is 1776.7419420831586\n",
      "the cost in 190000 step is 1775.447951354875\n",
      "the cost in 200000 step is 1774.2465536903944\n",
      "the cost in 0 step is 20576.308660686824\n",
      "the cost in 10000 step is 3444.047551910203\n",
      "the cost in 20000 step is 3377.4839662842332\n",
      "the cost in 30000 step is 3350.8809413137596\n",
      "the cost in 40000 step is 3335.833799625315\n",
      "the cost in 50000 step is 3326.089131294426\n",
      "the cost in 60000 step is 3319.2895856233527\n",
      "the cost in 70000 step is 3314.2954441476877\n",
      "the cost in 80000 step is 3310.478970401891\n",
      "the cost in 90000 step is 3307.4659237536894\n",
      "the cost in 100000 step is 3305.0206915983354\n",
      "the cost in 110000 step is 3302.9887299299826\n",
      "the cost in 120000 step is 3301.2652894855755\n",
      "the cost in 130000 step is 3299.7773979977815\n",
      "the cost in 140000 step is 3298.4730033611722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cost in 150000 step is 3297.314192991264\n",
      "the cost in 160000 step is 3296.272831530966\n",
      "the cost in 170000 step is 3295.3276825717717\n",
      "the cost in 180000 step is 3294.4624671361757\n",
      "the cost in 190000 step is 3293.664527990388\n",
      "the cost in 200000 step is 3292.923894195433\n",
      "the cost in 0 step is 21137.078491081204\n",
      "the cost in 10000 step is 3537.109339428206\n",
      "the cost in 20000 step is 3436.0548331330574\n",
      "the cost in 30000 step is 3398.3005635758823\n",
      "the cost in 40000 step is 3378.225014121994\n",
      "the cost in 50000 step is 3365.338215687117\n",
      "the cost in 60000 step is 3356.115688711854\n",
      "the cost in 70000 step is 3349.057324853924\n",
      "the cost in 80000 step is 3343.4112057817606\n",
      "the cost in 90000 step is 3338.7535731521043\n",
      "the cost in 100000 step is 3334.8239112069236\n",
      "the cost in 110000 step is 3331.451042581434\n",
      "the cost in 120000 step is 3328.516487313927\n",
      "the cost in 130000 step is 3325.934791665879\n",
      "the cost in 140000 step is 3323.642260327631\n",
      "the cost in 150000 step is 3321.590141850532\n",
      "the cost in 160000 step is 3319.740311377656\n",
      "the cost in 170000 step is 3318.062423498491\n",
      "the cost in 180000 step is 3316.5319681548644\n",
      "the cost in 190000 step is 3315.128902596308\n",
      "the cost in 200000 step is 3313.8366633633846\n",
      "Iteration took 4603.677029 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# calculate weight seperately\n",
    "learning_rate = 0.00002\n",
    "iteration_time = 200001\n",
    "tic = time.time()\n",
    "w0 = JS.gradient_descent1(y0_train, learning_rate, iteration_time)\n",
    "w1 = JS.gradient_descent1(y1_train, learning_rate, iteration_time)\n",
    "w2 = JS.gradient_descent1(y2_train, learning_rate, iteration_time)\n",
    "w3 = JS.gradient_descent1(y3_train, learning_rate, iteration_time)\n",
    "w4 = JS.gradient_descent1(y4_train, learning_rate, iteration_time)\n",
    "w5 = JS.gradient_descent1(y5_train, learning_rate, iteration_time)\n",
    "w6 = JS.gradient_descent1(y6_train, learning_rate, iteration_time)\n",
    "w7 = JS.gradient_descent1(y7_train, learning_rate, iteration_time)\n",
    "w8 = JS.gradient_descent1(y8_train, learning_rate, iteration_time)\n",
    "w9 = JS.gradient_descent1(y9_train, learning_rate, iteration_time)\n",
    "toc = time.time()\n",
    "print('Iteration took %f seconds' %(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWOLB0CSeIyQ"
   },
   "outputs": [],
   "source": [
    "# calculate probability for each category\n",
    "y_pred = []\n",
    "for i in range(np.shape(x_test)[0]):\n",
    "\n",
    "    pred_0 = JS.get_pcx(x_test[i], w0)\n",
    "    pred_1 = JS.get_pcx(x_test[i], w1)\n",
    "    pred_2 = JS.get_pcx(x_test[i], w2)\n",
    "    pred_3 = JS.get_pcx(x_test[i], w3)\n",
    "    pred_4 = JS.get_pcx(x_test[i], w4)\n",
    "    pred_5 = JS.get_pcx(x_test[i], w5)\n",
    "    pred_6 = JS.get_pcx(x_test[i], w6)\n",
    "    pred_7 = JS.get_pcx(x_test[i], w7)\n",
    "    pred_8 = JS.get_pcx(x_test[i], w8)\n",
    "    pred_9 = JS.get_pcx(x_test[i], w9)\n",
    "    pred = [pred_0[0],pred_1[0],pred_2[0],pred_3[0],pred_4[0],\n",
    "            pred_5[0],pred_6[0],pred_7[0],pred_8[0],pred_9[0]]\n",
    "    value = JS.predict_c(pred)\n",
    "    y_pred.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GXIOQxxeIyS",
    "outputId": "cce49998-7861-4dcd-fb8d-201f10772c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8775 / 10000 correct\n",
      "Accuracy = 0.877500\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NilrkASeIyb",
    "outputId": "efa9e2bb-019b-4543-e971-a5fce54df69a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 932    0    2    5    1   13   17    1    8    1]\n",
      " [   0 1098    5    2    0    3    4    3   20    0]\n",
      " [  11   15  877   22   18    9   15    9   53    3]\n",
      " [  13    2   36  854    1   45   11   10   24   14]\n",
      " [   3    4   16    1  888    4   14    8    9   35]\n",
      " [  19    6   11   59   22  674   23   12   53   13]\n",
      " [  20    3    6    2   13   22  879    3   10    0]\n",
      " [   2   14   23    7   13    4    0  915    6   44]\n",
      " [  15   11   17   27   16   32   14   14  818   10]\n",
      " [   8    7    1   14   52    9    4   58   16  840]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.911     0.951     0.931       980\n",
      "           1      0.947     0.967     0.957      1135\n",
      "           2      0.882     0.850     0.866      1032\n",
      "           3      0.860     0.846     0.853      1010\n",
      "           4      0.867     0.904     0.885       982\n",
      "           5      0.827     0.756     0.790       892\n",
      "           6      0.896     0.918     0.907       958\n",
      "           7      0.886     0.890     0.888      1028\n",
      "           8      0.804     0.840     0.822       974\n",
      "           9      0.875     0.833     0.853      1009\n",
      "\n",
      "   micro avg      0.877     0.877     0.877     10000\n",
      "   macro avg      0.876     0.875     0.875     10000\n",
      "weighted avg      0.877     0.877     0.877     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QPJpK6meIyd"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKZprlAHeIye"
   },
   "outputs": [],
   "source": [
    "# download MNIST dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# convert data type to float 32\n",
    "x_train=np.float32(x_train)[0:20000]\n",
    "y_train=y_train[0:20000]\n",
    "x_test=np.float32(x_test)\n",
    "x_train = x_train.reshape(np.shape(x_train)[0], 28*28)\n",
    "x_test = x_test.reshape(np.shape(x_test)[0], 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkGyh8AMGzbA"
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7qGQYXBeIyi"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200,criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-KjkLk2HB4Z"
   },
   "outputs": [],
   "source": [
    "# Train the model on training data\n",
    "rf.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FUFJ89SaeIyk",
    "outputId": "96c75963-478a-4c9e-97e3-5be92215bc62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7315\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Print out the mean square error (mse)\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wJ-wcHnueIyp",
    "outputId": "29bf5297-ebf4-41b7-97a7-15aa68782dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9608 / 10000 correct\n",
      "Accuracy = 0.960800\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "acc=rf.score(x_test, y_test)\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "k9ClVKt9eIyr",
    "outputId": "635e0045-df45-4012-dada-30c3f15cc29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.7432\n",
      "Accuracy on test set: 0.7471\n",
      "Accuracy on training set: 0.9260\n",
      "Accuracy on test set: 0.9168\n",
      "Accuracy on training set: 0.9841\n",
      "Accuracy on test set: 0.9469\n",
      "Accuracy on training set: 0.9988\n",
      "Accuracy on test set: 0.9580\n",
      "Accuracy on training set: 0.9999\n",
      "Accuracy on test set: 0.9573\n",
      "Accuracy on training set: 1.0000\n",
      "Accuracy on test set: 0.9593\n"
     ]
    }
   ],
   "source": [
    "md = [3,7,10,13,16,20]\n",
    "for i in range(len(md)):\n",
    "    rf1 = RandomForestClassifier(max_depth=md[i], \n",
    "                                 n_estimators=200,criterion = 'entropy')\n",
    "    rf1.fit(x_train, y_train)\n",
    "    print(\"Accuracy on training set: {:.4f}\".format(\n",
    "        rf1.score(x_train, y_train)))\n",
    "    print(\"Accuracy on test set: {:.4f}\".format(rf1.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ddL2DTLcITlD",
    "outputId": "f24c31fe-0ff9-4753-8183-c2a66fbe3170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.960000\n"
     ]
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier(max_depth=20, n_estimators=200,criterion = 'entropy')\n",
    "rf2.fit(x_train, y_train)\n",
    "y_pred = rf2.predict(x_test)\n",
    "acc=rf2.score(x_test, y_test)\n",
    "print('Accuracy = %f' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "9Xg_6OyceIyu",
    "outputId": "956d6c77-98f5-4bb0-bff5-55fd1139556e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 968    0    0    0    0    4    4    1    3    0]\n",
      " [   0 1122    3    3    0    2    2    1    2    0]\n",
      " [   6    0  987   10    3    0    7   11    7    1]\n",
      " [   1    0   11  968    0   10    0    9    7    4]\n",
      " [   1    0    2    0  944    0    8    0    3   24]\n",
      " [   5    1    0   22    3  843    8    2    6    2]\n",
      " [  11    3    0    0    7    4  931    0    2    0]\n",
      " [   1    6   24    1    3    0    0  979    4   10]\n",
      " [   5    1    6    6    5    6    9    5  917   14]\n",
      " [   5    6    3   14   15    4    2    3    8  949]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.965     0.988     0.976       980\n",
      "           1      0.985     0.989     0.987      1135\n",
      "           2      0.953     0.956     0.955      1032\n",
      "           3      0.945     0.958     0.952      1010\n",
      "           4      0.963     0.961     0.962       982\n",
      "           5      0.966     0.945     0.955       892\n",
      "           6      0.959     0.972     0.965       958\n",
      "           7      0.968     0.952     0.960      1028\n",
      "           8      0.956     0.941     0.949       974\n",
      "           9      0.945     0.941     0.943      1009\n",
      "\n",
      "   micro avg      0.961     0.961     0.961     10000\n",
      "   macro avg      0.961     0.960     0.960     10000\n",
      "weighted avg      0.961     0.961     0.961     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEbsU8FDeIyw"
   },
   "source": [
    "# PCA + Random Forest\n",
    "## PCA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IequJO5OeIyx"
   },
   "outputs": [],
   "source": [
    "# download MNIST dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# convert data type to float 32\n",
    "x_train=np.float32(x_train)\n",
    "x_test=np.float32(x_test)\n",
    "x_train = x_train.reshape(np.shape(x_train)[0], 28*28)/255.0\n",
    "x_test = x_test.reshape(np.shape(x_test)[0], 28*28)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRAfYjHSHhxP"
   },
   "outputs": [],
   "source": [
    "# stack them as a big one for dimension deduction\n",
    "big_X=np.vstack((x_train,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AluRCwncHhzw"
   },
   "outputs": [],
   "source": [
    "# build PCA class\n",
    "class SJPCA(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X):\n",
    "        self.x_train = X\n",
    "    \n",
    "    def compute_mean_covar_eigen(self):\n",
    "        # get average image and get mean image by summing each row\n",
    "        tr_mean = np.mean(self.x_train, axis=0)\n",
    "        tr_mean = np.reshape(tr_mean,(1,np.shape(tr_mean)[0]))\n",
    "\n",
    "        # subtract the mean\n",
    "        xtr_m = self.x_train - tr_mean\n",
    "\n",
    "        # calculate covariance matrix\n",
    "        tr_cov = np.dot(xtr_m.T,xtr_m)\n",
    "        \n",
    "        # get eigenvalue and eigenvector\n",
    "        tr_val, tr_vec = eig(tr_cov)\n",
    "        \n",
    "        return xtr_m, tr_cov, tr_val, tr_vec\n",
    "    \n",
    "    def get_comp_K(self,tr_val, threshold):\n",
    "        cum_lambda = np.cumsum(tr_val)\n",
    "        total_lamda = cum_lambda[-1]\n",
    "        \n",
    "        # get the principal component number that we want to keep\n",
    "        for keep_dim in range(len(tr_val)):\n",
    "            rate = cum_lambda[keep_dim]/total_lamda\n",
    "            if rate >= threshold: \n",
    "                return keep_dim\n",
    "                break\n",
    "            else: continue\n",
    "            \n",
    "    def deduct_img(self, tr_vec, keep_dim):\n",
    "        x_proj= np.dot(self.x_train, tr_vec.T[:,0:keep_dim])\n",
    "        return x_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjOPnG8ieIy0"
   },
   "source": [
    "## Retain 90% Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7GdJQWy1eIy1",
    "outputId": "135d16b0-2eec-4a48-d2e8-d8bda74742dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "# Deduct Training Set\n",
    "SJ = SJPCA()\n",
    "SJ.train(big_X)\n",
    "xtr_m, tr_cov, tr_val, tr_vec = SJ.compute_mean_covar_eigen()\n",
    "keep_dim = SJ.get_comp_K(tr_val, 0.90)\n",
    "new_big_X = SJ.deduct_img(tr_vec, keep_dim)\n",
    "print(keep_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62xHbqF8eIy2"
   },
   "outputs": [],
   "source": [
    "# resplit the dataset and normalize them with min-max normalization\n",
    "x_train = new_big_X[0:60000,:]\n",
    "x_test = new_big_X[60000:70000,:]\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZMr8IY_iH54k",
    "outputId": "249224bf-b26c-4e77-d5e6-c81a1071d05a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.2954\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200,criterion = 'entropy')\n",
    "# Train the model on training data\n",
    "rf.fit(x_train[0:20000,], y_train[0:20000,])\n",
    "from sklearn import metrics\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test[0:20000,])\n",
    "\n",
    "# Print out the mean square error (mse)\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q5za5_GZH57D",
    "outputId": "3231a3e4-b9a0-4f10-c04d-a5c4ffdee539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7912 / 10000 correct\n",
      "Accuracy = 0.791200\n"
     ]
    }
   ],
   "source": [
    " # calculate accuracy\n",
    "acc=rf.score(x_test, y_test)\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "5x1MHXBFeIy3",
    "outputId": "74ade9e9-68d8-4d47-8add-94253f3cf118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.506\n",
      "Accuracy on test set: 0.503\n",
      "Accuracy on training set: 0.700\n",
      "Accuracy on test set: 0.670\n",
      "Accuracy on training set: 0.859\n",
      "Accuracy on test set: 0.737\n",
      "Accuracy on training set: 0.979\n",
      "Accuracy on test set: 0.773\n",
      "Accuracy on training set: 0.998\n",
      "Accuracy on test set: 0.787\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.788\n"
     ]
    }
   ],
   "source": [
    "md = [3,7,10,13,16,20]\n",
    "for i in range(len(md)):\n",
    "    rf1 = RandomForestClassifier(max_depth=md[i], \n",
    "                                 n_estimators=200,criterion = 'entropy')\n",
    "    rf1.fit(x_train[0:20000,], y_train[0:20000,])\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(rf1.score(\n",
    "          x_train[0:20000,], y_train[0:20000,])))\n",
    "    print(\"Accuracy on test set: {:.3f}\".format(rf1.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "iVuApmlxH-rh",
    "outputId": "7015bf14-89b2-4d7f-da62-cf4b89bb5505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 848    0   16   13    2   12   56    0   33    0]\n",
      " [   0 1089   13    2    4    0   10    1   16    0]\n",
      " [  34    9  817   24    5    6   67    4   66    0]\n",
      " [  27    1   36  778   13   57   27   12   49   10]\n",
      " [   0   20   19    3  819    1   16   18    5   81]\n",
      " [  33    5   33  241   18  439   30    6   78    9]\n",
      " [  20    4   37    0   15   11  846    0   25    0]\n",
      " [   0   26    4    2   54    0    5  818   10  109]\n",
      " [  62   14   16   38    7   24   21    9  777    6]\n",
      " [   4    6   10   17  113    9    0  170    6  674]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.825     0.865     0.845       980\n",
      "           1      0.928     0.959     0.943      1135\n",
      "           2      0.816     0.792     0.804      1032\n",
      "           3      0.696     0.770     0.731      1010\n",
      "           4      0.780     0.834     0.806       982\n",
      "           5      0.785     0.492     0.605       892\n",
      "           6      0.785     0.883     0.831       958\n",
      "           7      0.788     0.796     0.792      1028\n",
      "           8      0.730     0.798     0.762       974\n",
      "           9      0.758     0.668     0.710      1009\n",
      "\n",
      "   micro avg      0.790     0.790     0.790     10000\n",
      "   macro avg      0.789     0.786     0.783     10000\n",
      "weighted avg      0.791     0.790     0.787     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLa4YLAgeIy5"
   },
   "source": [
    "## Retain 95% Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tLvhzIdxeIy5",
    "outputId": "226bf67a-ab4e-4ab9-a21e-3e7309dca4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "# Deduct Training Set\n",
    "SJ = SJPCA()\n",
    "SJ.train(big_X)\n",
    "xtr_m, tr_cov, tr_val, tr_vec = SJ.compute_mean_covar_eigen()\n",
    "keep_dim = SJ.get_comp_K(tr_val, 0.95)\n",
    "new_big_X = SJ.deduct_img(tr_vec, keep_dim)\n",
    "print(keep_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAingGHlIwpB"
   },
   "outputs": [],
   "source": [
    "# resplit the dataset and normalize them with min-max normalization\n",
    "x_train = new_big_X[0:60000,:]\n",
    "x_test = new_big_X[60000:70000,:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X362L8y-eIy6",
    "outputId": "ba1c8a93-9dce-4cfc-bb7e-b51dca6842eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.7849\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200,criterion = 'entropy')\n",
    "# Train the model on training data\n",
    "rf.fit(x_train[0:20000,], y_train[0:20000,])\n",
    "from sklearn import metrics\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test[0:20000,])\n",
    "\n",
    "# Print out the mean square error (mse)\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "c6dYa8PvIwtc",
    "outputId": "1ecb4931-be36-4e67-ed9a-db20d0e65c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8928 / 10000 correct\n",
      "Accuracy = 0.892800\n"
     ]
    }
   ],
   "source": [
    " # calculate accuracy\n",
    "acc=rf.score(x_test, y_test)\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Q04oiKkbIwwG",
    "outputId": "c3b0f212-f515-46f9-ddc3-84a9c00cdc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.628\n",
      "Accuracy on test set: 0.621\n",
      "Accuracy on training set: 0.822\n",
      "Accuracy on test set: 0.798\n",
      "Accuracy on training set: 0.956\n",
      "Accuracy on test set: 0.864\n",
      "Accuracy on training set: 0.999\n",
      "Accuracy on test set: 0.885\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.892\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.891\n"
     ]
    }
   ],
   "source": [
    "md = [3,7,10,13,16,20]\n",
    "for i in range(len(md)):\n",
    "    rf1 = RandomForestClassifier(max_depth=md[i], \n",
    "                                 n_estimators=200,criterion = 'entropy')\n",
    "    rf1.fit(x_train[0:20000,], y_train[0:20000,])\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(rf1.score(\n",
    "         x_train[0:20000,], y_train[0:20000,])))\n",
    "    print(\"Accuracy on test set: {:.3f}\".format(rf1.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "x8h2QhUSJGyo",
    "outputId": "55013376-09b7-4bfc-f919-e0441948cb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 908    0    7    3    1    5   44    1   11    0]\n",
      " [   0 1111    5    3    2    1    5    0    8    0]\n",
      " [  11    3  923   19   10    1   27    7   31    0]\n",
      " [  12    0   21  884    5   37   19   11   16    5]\n",
      " [   0    4   10    0  899    1    8   10    4   46]\n",
      " [  25    3    6   73   18  688   26    5   34   14]\n",
      " [  21    1   15    1   12    7  879    1   21    0]\n",
      " [   0   12   12    0   32    0    3  912   10   47]\n",
      " [  18    2   12   21    6   30   16    5  857    7]\n",
      " [   4    7    2   21   73    4    1   27    3  867]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.909     0.927     0.918       980\n",
      "           1      0.972     0.979     0.975      1135\n",
      "           2      0.911     0.894     0.903      1032\n",
      "           3      0.862     0.875     0.869      1010\n",
      "           4      0.850     0.915     0.881       982\n",
      "           5      0.889     0.771     0.826       892\n",
      "           6      0.855     0.918     0.885       958\n",
      "           7      0.932     0.887     0.909      1028\n",
      "           8      0.861     0.880     0.870       974\n",
      "           9      0.879     0.859     0.869      1009\n",
      "\n",
      "   micro avg      0.893     0.893     0.893     10000\n",
      "   macro avg      0.892     0.891     0.891     10000\n",
      "weighted avg      0.894     0.893     0.892     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Methods_Comparison_via_Handwritten_Recognition_Dataset.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
