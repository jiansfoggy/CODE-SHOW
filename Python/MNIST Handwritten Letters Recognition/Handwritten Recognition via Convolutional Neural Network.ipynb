{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0F9ZJcXkvV6"
   },
   "source": [
    "<h4 align=\"center\">Jian Sun</h4>\n",
    "<h4 align=\"center\">DUID: 873397832</h4>\n",
    "\n",
    "# Content\n",
    "- Introduction\n",
    "- Model\n",
    "- Result\n",
    "  - Train VGG-16 with MNIST from Scratch\n",
    "  - Predict MNIST by Trained Imagenet Weight in VGG-16\n",
    "  - Freeze All The Convolutional Layers Retrain VGG-16 Network On MNIST\n",
    "  - Freeze All The Fully Connected Layers Retrain VGG-16 Network On MNIST\n",
    "- Discussion\n",
    "- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcNCcwyrl_19"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we implement two methods to recognize MNIST dataset. \n",
    "\n",
    "One is to code the VGG-16 from scratch and train it with the MNIST from the very beginning, another method is to train MNISR with existing VGG-16 weight drawn from Imagenet dataset. Specifically, we will discuss the prediction capability after freezing all convolutional layers or freezing all fully connected layers. This will help us to learn the power of different layers in convolutional neural network. Before doing the experiment, we hypothesize that fully connected layer is vital crucial. Without it, the whole nerual network will perform extremely worse. Conversely, convolional layer doesn't play an important layer for MNIST dataset, since it is a binery image dataset, not the colorful one. \n",
    "\n",
    "# Model\n",
    "## Data Processing\n",
    "The whole MNIST is splitted as the following three parts.\n",
    "\n",
    "The training set has 48000  28X28 images;  \n",
    "the validation set has 12000 28X28 images;  \n",
    "the testing set has 10000 28X28 images.  \n",
    "\n",
    "Then, given that the VGG-16 doesn't accept image which size is smaller than 32X32, the current images have to be enlarged. The method here is adding paddings. And the final size is 34X34, which fits for VGG-16 model.\n",
    "\n",
    "## Train VGG-16 with MNIST from Scratch\n",
    "This part starts from stacking layers. With the help of Sequence() from keras, we build the network up. And we set the batch size as 16, epoch as 500, optimizer as adam, loss as categorical cross entropy loss.\n",
    "\n",
    "Callbacks funciton is used here to reduce the learning rate automatically and stop the training when validation loss doesn't change for 10 epochs, this function saves us lots of time and is very convenient.\n",
    "\n",
    "## Predict MNIST by Trained Imagenet Weight in VGG-16\n",
    "A very powerful function, VGG16() from keras.applications helps to construct the neural network here. And we set the batch size as 16, epoch as 100, optimizer as adam, loss as categorical cross entropy loss.\n",
    " \n",
    "Callbacks funciton is also used here to reduce the learning rate automatically and stop the training when validation loss doesn't change for 8 epochs, this function saves us lots of time and is very convenient.\n",
    " \n",
    "## Freeze All The Convolutional Layers Retrain VGG-16 Network On MNIST\n",
    "A very powerful function, VGG16() from keras.applications helps to construct the neural network here. And we set the batch size as 16, epoch as 100, optimizer as adam, loss as categorical cross entropy loss. Additionally, the convolutional layers are frozen by code: layer.trainable = False.\n",
    " \n",
    "Callbacks funciton is also used here to reduce the learning rate automatically and stop the training when validation loss doesn't change for 8 epochs, this function saves us lots of time and is very convenient.\n",
    "\n",
    "## Freeze All The Fully Connected Layers Retrain VGG-16 Network On MNIST\n",
    "A very powerful function, VGG16() from keras.applications helps to construct the neural network here. And we set the batch size as 16, epoch as 100, optimizer as adam, loss as categorical cross entropy loss. Additionally, the fully connected layers are frozen by code: layer.trainable = False.\n",
    " \n",
    "Callbacks funciton is also used here to reduce the learning rate automatically and stop the training when validation loss doesn't change for 8 epochs, this function saves us lots of time and is very convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJB4BgLD-Yra"
   },
   "source": [
    "# Train VGG-16 with MNIST from Scratch\n",
    "\n",
    "The training stops at epoach 11 with the training loss of 4.9966e-04, the training accuracy of 0.9999, the validation loss of 0.0688 and the validation accuracy of 0.9934, when the learning rate is 7.999999979801942e-07.\n",
    "\n",
    "Next, we use this weight to predict testing set, the prediction loss is 0.0492 and the prediction accuracy is 0.9953.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "colab_type": "code",
    "id": "UNp168BK-rge",
    "outputId": "7d4ad453-7f17-41f3-f357-eeca64eb837c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9953 / 10000 correct\n",
      "Accuracy = 0.995300\n",
      "[[ 977    0    0    0    0    0    2    1    0    0]\n",
      " [   0 1130    1    2    0    0    0    1    1    0]\n",
      " [   0    0 1029    1    0    0    0    2    0    0]\n",
      " [   0    0    0 1006    0    3    0    0    1    0]\n",
      " [   0    0    0    0  977    0    1    0    0    4]\n",
      " [   0    0    0    5    0  886    1    0    0    0]\n",
      " [   4    2    0    0    0    0  952    0    0    0]\n",
      " [   0    2    1    0    0    0    0 1024    0    1]\n",
      " [   0    0    0    1    0    0    0    0  972    1]\n",
      " [   0    0    0    0    4    1    0    2    2 1000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.996     0.997     0.996       980\n",
      "           1      0.996     0.996     0.996      1135\n",
      "           2      0.998     0.997     0.998      1032\n",
      "           3      0.991     0.996     0.994      1010\n",
      "           4      0.996     0.995     0.995       982\n",
      "           5      0.996     0.993     0.994       892\n",
      "           6      0.996     0.994     0.995       958\n",
      "           7      0.994     0.996     0.995      1028\n",
      "           8      0.996     0.998     0.997       974\n",
      "           9      0.994     0.991     0.993      1009\n",
      "\n",
      "    accuracy                          0.995     10000\n",
      "   macro avg      0.995     0.995     0.995     10000\n",
      "weighted avg      0.995     0.995     0.995     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA5ReAQJFO9TCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnsD1TFx-e6n"
   },
   "source": [
    "# Predict MNIST by Trained Imagenet Weight in VGG-16\n",
    "\n",
    "The training stops at epoach 37 with the training loss of 0.0303, the training accuracy of 0.9920, the validation loss of 0.0724 and the validation accuracy of 0.9778, when the learning rate is 6.400000529538374e-08.\n",
    "\n",
    "Next, we use this weight to predict testing set, the prediction loss is 0.0672 and the prediction accuracy is 0.9788."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "colab_type": "code",
    "id": "EI0CTb9K-w64",
    "outputId": "e30cab72-c64a-4b3f-bfc8-c2214c7912df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9788 / 10000 correct\n",
      "Accuracy = 0.978800\n",
      "[[ 969    0    1    0    0    4    5    1    0    0]\n",
      " [   0 1123    0    0    3    0    4    4    1    0]\n",
      " [   1    1 1008    4    2    4    4    5    2    1]\n",
      " [   0    0   10  977    0   16    0    3    2    2]\n",
      " [   0    1    1    0  967    1    2    3    4    3]\n",
      " [   2    0    7    9    1  864    0    2    6    1]\n",
      " [   7    0    4    0    0    2  944    0    1    0]\n",
      " [   0    5    4    2    8    0    0 1004    1    4]\n",
      " [   0    0    6    2    3    4    1    1  956    1]\n",
      " [   3    0    3    2    7    2    0    5   11  976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.989     0.988       980\n",
      "           1      0.994     0.989     0.992      1135\n",
      "           2      0.966     0.977     0.971      1032\n",
      "           3      0.981     0.967     0.974      1010\n",
      "           4      0.976     0.985     0.980       982\n",
      "           5      0.963     0.969     0.966       892\n",
      "           6      0.983     0.985     0.984       958\n",
      "           7      0.977     0.977     0.977      1028\n",
      "           8      0.972     0.982     0.977       974\n",
      "           9      0.988     0.967     0.977      1009\n",
      "\n",
      "    accuracy                          0.979     10000\n",
      "   macro avg      0.979     0.979     0.979     10000\n",
      "weighted avg      0.979     0.979     0.979     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3rsgusE-fJR"
   },
   "source": [
    "# Freeze All The Convolutional Layers Retrain VGG-16 Network On MNIST\n",
    "\n",
    "The training stops at epoach 22 with the training loss of 0.0272, the training accuracy of 0.9924, the validation loss of 0.0723 and the validation accuracy of 0.9788, when the learning rate is 6.400000529538374e-08.\n",
    "\n",
    "Next, we use this weight to predict testing set, the prediction loss is 0.0693 and the prediction accuracy is 0.9771."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "colab_type": "code",
    "id": "J_45etwMFgYa",
    "outputId": "5a2c37ea-9da6-4bfc-b9bf-655b48e453d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9771 / 10000 correct\n",
      "Accuracy = 0.977100\n",
      "[[ 967    0    1    0    0    3    5    1    2    1]\n",
      " [   0 1125    0    0    3    0    4    2    1    0]\n",
      " [   1    2 1000    8    2    6    3    6    3    1]\n",
      " [   0    0    9  976    0   18    0    4    2    1]\n",
      " [   0    1    0    0  968    1    3    2    3    4]\n",
      " [   2    0    4   13    1  861    3    2    5    1]\n",
      " [   7    1    2    0    1    3  941    0    2    1]\n",
      " [   0    3    4    3   10    0    0 1002    1    5]\n",
      " [   0    0    3    4    2    4    2    2  955    2]\n",
      " [   3    0    3    3    5    3    0    4   12  976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.987     0.987       980\n",
      "           1      0.994     0.991     0.993      1135\n",
      "           2      0.975     0.969     0.972      1032\n",
      "           3      0.969     0.966     0.968      1010\n",
      "           4      0.976     0.986     0.981       982\n",
      "           5      0.958     0.965     0.961       892\n",
      "           6      0.979     0.982     0.981       958\n",
      "           7      0.978     0.975     0.976      1028\n",
      "           8      0.969     0.980     0.974       974\n",
      "           9      0.984     0.967     0.976      1009\n",
      "\n",
      "    accuracy                          0.977     10000\n",
      "   macro avg      0.977     0.977     0.977     10000\n",
      "weighted avg      0.977     0.977     0.977     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1c7aT3n-fSR"
   },
   "source": [
    "# Freeze All The Fully Connected Layers Retrain VGG-16 Network On MNIST\n",
    "\n",
    "The training stops at epoach 9 with the training loss of 2.4916, the training accuracy of 0.1330, the validation loss of 2.5009 and the validation accuracy of 0.1352, when the learning rate is 8.000000525498762e-06.\n",
    "\n",
    "Next, we use this weight to predict testing set, the prediction loss is 2.4918 and the prediction accuracy is  0.1296.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1022
    },
    "colab_type": "code",
    "id": "lw2IsWE0_r0B",
    "outputId": "e54c1e3b-6e15-4447-adfd-144b2f917a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1296 / 10000 correct\n",
      "Accuracy = 0.129600\n",
      "[[   0    0    0    0    0    0    0    0  980    0]\n",
      " [   0    0    0    4  821  266    0    2   34    8]\n",
      " [   0    0    0    0    5    0    0   10 1015    2]\n",
      " [   0    0    0    0    4    0    0    0 1005    1]\n",
      " [   0    0    0    5  315   27    0   12  612   11]\n",
      " [   0    0    0    0    8    0    0    2  881    1]\n",
      " [   0    0    0    0    8    1    0    3  939    7]\n",
      " [   0    0    0    0  217   48    0    8  702   53]\n",
      " [   0    0    0    0    2    0    0    0  972    0]\n",
      " [   0    0    0    0   15    5    0    0  988    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       980\n",
      "           1      0.000     0.000     0.000      1135\n",
      "           2      0.000     0.000     0.000      1032\n",
      "           3      0.000     0.000     0.000      1010\n",
      "           4      0.226     0.321     0.265       982\n",
      "           5      0.000     0.000     0.000       892\n",
      "           6      0.000     0.000     0.000       958\n",
      "           7      0.216     0.008     0.015      1028\n",
      "           8      0.120     0.998     0.214       974\n",
      "           9      0.012     0.001     0.002      1009\n",
      "\n",
      "    accuracy                          0.130     10000\n",
      "   macro avg      0.057     0.133     0.050     10000\n",
      "weighted avg      0.057     0.130     0.049     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHKCAYAAABhQBClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlkyakiS19P8BiQYe9IcKxpEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classes = list(label_dict.values())\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50EUhp9Z_WhZ"
   },
   "source": [
    "\n",
    " \n",
    "## Discussion\n",
    "\n",
    "Here is a detailed table for all 4 parts.\n",
    "\n",
    "Methods           | Scratch  | Retrain | Freeze Conv | Freeze Fully Con\n",
    "------------------|----------|---------|-------------|---------------------------\n",
    "Train Acc         | 99.99%   | 99.20%  | 99.24%      | 13.30%\n",
    "Train Loss        | 5e^-4    | 0.0303  | 0.0272      | 2.4916 \n",
    "Valid Acc         | 99.34%   | 97.78%  | 97.88%      | 13.52%\n",
    "Valid Loss        | 0.0688   | 0.0724  | 0.0723      | 2.5009\n",
    "Test Acc          | 99.53%   | 97.88%  | 97.71%      | 12.96%\n",
    "Test Loss         | 0.0492   | 0.0672  | 0.0693      | 2.4918\n",
    "Time Per Epoch    | 143s     | 5s      | 5.5s        | 3s\n",
    "Learning Rate     | 8e^-7    | 6e^-8   | 6e^-8       | 8e^-6\n",
    "\n",
    "As we can see, the total rank for training part is Scratch > Freeze Conv > Retrain > Freeze Fully Con. Scratch performs best on training part. VGG-16 freezing all convolutional layers performs bettern than Retrain it on MNIST. It is a little weird, but it is truth. Meanwhile, VGG-16 freezing all full connected layers performs worst. \n",
    "\n",
    "The total rank for testing part is Scratch > Retrain > Freeze Conv >  Freeze Fully Con. \n",
    "\n",
    "In general, from this comparison, we recognize that full connected layer is so important for CNN, however, if we close convolutional layer, our result will not be influenced a lot. Perhaps, it is due to MNIST dataset as the binery image dataset, not the colorful one. Thus lossing convoluitonal layers doesn't affect a lot on CNN's performance. However, lossing fully connected layer means different, it will ruin your prediction, since we need softmax to do classification.\n",
    "\n",
    "CNN is a powerful model, there is huge CNN families. We hope to dig more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55I9HtBu_sHa"
   },
   "source": [
    "# Appendix\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjJiOc-7zofs"
   },
   "outputs": [],
   "source": [
    "# import dataset and seperate them as train set and test set\n",
    "# index x represents image, index y represents label\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from numpy.linalg import *\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.convolutional import *\n",
    "from keras import callbacks, layers, optimizers, models\n",
    "import os, cv2, random, sklearn, sklearn.metrics, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Activation, Dropout, Flatten\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxgQsHu0p-6M"
   },
   "source": [
    "## Load Dataset and Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ztGd5-h0o9f"
   },
   "outputs": [],
   "source": [
    "# download MNIST dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "y_train_labels = to_categorical(y_train)\n",
    "y_test_labels = to_categorical(y_test)\n",
    "# make sure the 10 classes\n",
    "label_dict = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4',\n",
    "              5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "id": "bo09mkui4HvG",
    "outputId": "4a065873-1ce6-41f9-ccc5-ac0123f24edc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAI/CAYAAACf7mYiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABAOn8DGSJKZBz6uA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify that the data is in the correct format.\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(label_dict[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "6urIsf-N-TUt",
    "outputId": "b94bb674-0cb0-49bf-c422-e2610aaed4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784, 3)\n",
      "(10000, 784, 3)\n",
      "(60000, 28, 28, 3)\n",
      "(10000, 28, 28, 3)\n",
      "(60000, 34, 34, 3)\n",
      "(10000, 34, 34, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(np.shape(x_train)[0], 28*28)\n",
    "x_test = x_test.reshape(np.shape(x_test)[0], 28*28)\n",
    "# Convert the images into 3 channels\n",
    "X_train=np.dstack([x_train] * 3)\n",
    "X_test=np.dstack([x_test] * 3)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "# Reshape images as per the tensor format required by tensorflow\n",
    "X_train = X_train.reshape(-1, 28,28,3)\n",
    "X_test = X_test.reshape (-1,28,28,3)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "# Resize the images 34*34 as required by VGG16\n",
    "X_tr = np.pad(X_train, ((0,0),(3,3),(3,3),(0,0)), 'constant')\n",
    "X_te = np.pad(X_test, ((0,0),(3,3),(3,3),(0,0)), 'constant')\n",
    "X_tr = np.float32(X_tr)/255.\n",
    "X_te = np.float32(X_te)/255.\n",
    "print(np.shape(X_tr))\n",
    "print(np.shape(X_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ngCmWp9W0pAO",
    "outputId": "21a97174-a7b3-4fdd-c1b8-19d9d188bf11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 34, 34, 3), (12000, 34, 34, 3), (48000, 10), (12000, 10))"
      ]
     },
     "execution_count": 187,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Splitting train data as train and validation data\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(X_tr,\n",
    "                                                           y_train_labels,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=13)\n",
    "# Finally check the data size whether it is as per tensorflow and VGG16 requirement\n",
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06g9MSBiDu9_"
   },
   "outputs": [],
   "source": [
    "# Define the parameters for instanitaing VGG16 model. \n",
    "IMG_WIDTH = 34\n",
    "IMG_HEIGHT = 34\n",
    "IMG_DEPTH = 3\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfyYpgL6HESK"
   },
   "source": [
    "## Train VGG-16 with MNIST from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "j_jhH8h3C0Qe",
    "outputId": "91004e25-04b6-4049-b421-de95d5ec0b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 34, 34, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 17, 17, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 33,597,248\n",
      "Trainable params: 33,597,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(IMG_WIDTH, IMG_HEIGHT, IMG_DEPTH))\n",
    "#input_tensor = input_tensor\n",
    "vg16_m = VGG16(weights=None,input_tensor = input_tensor)\n",
    "type(vg16_m)\n",
    "model = Sequential()\n",
    "for layer in vg16_m.layers[0:22]:\n",
    "  model.add(layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "r0YlyZjUexN4",
    "outputId": "03b259de-da0a-4838-a9d3-4e8b0fd613f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 34, 34, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 17, 17, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 33,638,218\n",
      "Trainable params: 33,638,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dzc3fS4F_8J"
   },
   "outputs": [],
   "source": [
    "NB_EPOCHS = 100\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(Adam(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "GFHYELCWGLrF",
    "outputId": "9f6e7343-4d1f-4c7a-9a95-18ce9f41da8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "# Incorporating reduced learning and early stopping for callback\n",
    "reduce_learning = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    epsilon=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=0)\n",
    "\n",
    "eary_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0003,\n",
    "    patience=6,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "uTQuyDzylpe8",
    "outputId": "510b0961-babf-44b7-fa86-5f6671bc9c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      " - 147s - loss: 0.0311 - acc: 0.9933 - val_loss: 0.0585 - val_acc: 0.9894\n",
      "Epoch 2/100\n",
      " - 144s - loss: 0.0332 - acc: 0.9933 - val_loss: 0.0518 - val_acc: 0.9914\n",
      "Epoch 3/100\n",
      " - 144s - loss: 0.0261 - acc: 0.9943 - val_loss: 0.0554 - val_acc: 0.9905\n",
      "Epoch 4/100\n",
      " - 143s - loss: 0.0269 - acc: 0.9948 - val_loss: 0.0533 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 5/100\n",
      " - 143s - loss: 0.0080 - acc: 0.9984 - val_loss: 0.0374 - val_acc: 0.9930\n",
      "Epoch 6/100\n",
      " - 143s - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0491 - val_acc: 0.9936\n",
      "Epoch 7/100\n",
      " - 143s - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0528 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "Epoch 8/100\n",
      " - 143s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0550 - val_acc: 0.9934\n",
      "Epoch 9/100\n",
      " - 143s - loss: 7.9653e-04 - acc: 0.9999 - val_loss: 0.0622 - val_acc: 0.9932\n",
      "Epoch 10/100\n",
      " - 143s - loss: 6.3383e-04 - acc: 0.9999 - val_loss: 0.0677 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "Epoch 11/100\n",
      " - 143s - loss: 4.9966e-04 - acc: 0.9999 - val_loss: 0.0688 - val_acc: 0.9934\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "mt=model.fit(train_X, train_label, \n",
    "             batch_size=16, epochs=NB_EPOCHS,\n",
    "             verbose=2, callbacks=callbacks,\n",
    "             validation_data=(valid_X, valid_label)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "-t4Z8lyS191N",
    "outputId": "02dac7ca-4bc8-45fa-8c19-46b99f575b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 310us/step\n",
      "Test Accuracy:0.995300, Test Loss:0.049209.\n",
      "Got 9953 / 10000 correct\n",
      "Accuracy = 0.995300\n",
      "[[ 977    0    0    0    0    0    2    1    0    0]\n",
      " [   0 1130    1    2    0    0    0    1    1    0]\n",
      " [   0    0 1029    1    0    0    0    2    0    0]\n",
      " [   0    0    0 1006    0    3    0    0    1    0]\n",
      " [   0    0    0    0  977    0    1    0    0    4]\n",
      " [   0    0    0    5    0  886    1    0    0    0]\n",
      " [   4    2    0    0    0    0  952    0    0    0]\n",
      " [   0    2    1    0    0    0    0 1024    0    1]\n",
      " [   0    0    0    1    0    0    0    0  972    1]\n",
      " [   0    0    0    0    4    1    0    2    2 1000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.996     0.997     0.996       980\n",
      "           1      0.996     0.996     0.996      1135\n",
      "           2      0.998     0.997     0.998      1032\n",
      "           3      0.991     0.996     0.994      1010\n",
      "           4      0.996     0.995     0.995       982\n",
      "           5      0.996     0.993     0.994       892\n",
      "           6      0.996     0.994     0.995       958\n",
      "           7      0.994     0.996     0.995      1028\n",
      "           8      0.996     0.998     0.997       974\n",
      "           9      0.994     0.991     0.993      1009\n",
      "\n",
      "    accuracy                          0.995     10000\n",
      "   macro avg      0.995     0.995     0.995     10000\n",
      "weighted avg      0.995     0.995     0.995     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_te, y_test_labels)\n",
    "print('Test Accuracy:%4f, Test Loss:%4f.' %(test_acc,test_loss))\n",
    "# Make Prediction\n",
    "pred_result = model.predict(X_te)\n",
    "y_pred=[]\n",
    "for i in range(np.shape(y_test)[0]):\n",
    "  num = np.where(pred_result[i]==max(pred_result[i]))\n",
    "  y_pred.append(num[0][0])\n",
    "y_pred = np.transpose(y_pred)\n",
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "essqKXG_HEjj"
   },
   "source": [
    "## Predict MNIST by Trained Imagenet Weight in VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UEerYffeHSI_",
    "outputId": "1104767d-a55b-419c-c77a-c09e36d58b56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#conv_base.summary()\\nmodel = Sequential()\\nfor layer in conv_base.layers:\\n          model.add(layer)\\n          \\nmodel.summary()\\n'"
      ]
     },
     "execution_count": 195,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Create base model of VGG16\n",
    "from keras.layers import Input\n",
    "input_tensor = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH))\n",
    "conv_base = VGG16(weights='imagenet',include_top=False, \n",
    "                  input_tensor = input_tensor)\n",
    "\n",
    "type(conv_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "EvtaUPifJyrC",
    "outputId": "e692ef7c-1055-4f88-c44b-03e472489332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 18s 381us/step\n",
      "10000/10000 [==============================] - 4s 381us/step\n",
      "12000/12000 [==============================] - 5s 383us/step\n"
     ]
    }
   ],
   "source": [
    " # Extracting features\n",
    "train_features = conv_base.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "test_features = conv_base.predict(np.array(X_te), batch_size=BATCH_SIZE, verbose=1)\n",
    "val_features = conv_base.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eWK-BTxKzyx"
   },
   "outputs": [],
   "source": [
    "np.savez(\"train_features\", train_features, train_label)\n",
    "np.savez(\"test_features\", test_features, y_test_labels)\n",
    "np.savez(\"val_features\", val_features, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zWKSOjp0Kz17",
    "outputId": "4cded49d-ca91-449e-8d44-9865c7cd3e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 1, 1, 512) \n",
      " (10000, 1, 1, 512) \n",
      " (12000, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "# Current shape of features\n",
    "print(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjiP84cGJyu5"
   },
   "outputs": [],
   "source": [
    "# Flatten extracted features\n",
    "train_features_flat = np.reshape(train_features, (48000, 1*1*512))\n",
    "test_features_flat = np.reshape(test_features, (10000, 1*1*512))\n",
    "val_features_flat = np.reshape(val_features, (12000, 1*1*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QHh4MbTCzbQ"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "model_c = models.Sequential()\n",
    "model_c.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
    "model_c.add(layers.LeakyReLU(alpha=0.1))\n",
    "model_c.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60zT5TUe5jy3"
   },
   "outputs": [],
   "source": [
    "NB_TRAIN_SAMPLES = train_features_flat.shape[0]\n",
    "NB_VALIDATION_SAMPLES = val_features_flat.shape[0]\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "# Compile the model.\n",
    "model_c.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "2ike8OcpMj9p",
    "outputId": "fd84bbbd-ae51-4691-ac2f-f622cb286eed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# Incorporating reduced learning and early stopping for callback\n",
    "reduce_learning = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    epsilon=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=0)\n",
    "\n",
    "eary_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0003,\n",
    "    patience=8,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1513
    },
    "colab_type": "code",
    "id": "NtWwCFx86ZSM",
    "outputId": "b2ed4e31-d2ca-40f7-cd0c-f747b1a29361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.3400 - acc: 0.8962 - val_loss: 0.1871 - val_acc: 0.9385\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.1631 - acc: 0.9472 - val_loss: 0.1368 - val_acc: 0.9557\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.1389 - acc: 0.9547 - val_loss: 0.1327 - val_acc: 0.9575\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.1198 - acc: 0.9608 - val_loss: 0.1475 - val_acc: 0.9511\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.1087 - acc: 0.9643 - val_loss: 0.1182 - val_acc: 0.9625\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.1028 - acc: 0.9663 - val_loss: 0.1109 - val_acc: 0.9652\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0941 - acc: 0.9685 - val_loss: 0.0994 - val_acc: 0.9696\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0897 - acc: 0.9692 - val_loss: 0.1261 - val_acc: 0.9606\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0842 - acc: 0.9719 - val_loss: 0.1133 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0556 - acc: 0.9821 - val_loss: 0.0820 - val_acc: 0.9729\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0524 - acc: 0.9832 - val_loss: 0.0829 - val_acc: 0.9738\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0505 - acc: 0.9837 - val_loss: 0.0814 - val_acc: 0.9740\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0493 - acc: 0.9841 - val_loss: 0.0820 - val_acc: 0.9745\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0478 - acc: 0.9847 - val_loss: 0.0800 - val_acc: 0.9752\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0457 - acc: 0.9854 - val_loss: 0.0784 - val_acc: 0.9773\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0447 - acc: 0.9860 - val_loss: 0.0766 - val_acc: 0.9761\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.0432 - acc: 0.9863 - val_loss: 0.0760 - val_acc: 0.9761\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.0425 - acc: 0.9867 - val_loss: 0.0774 - val_acc: 0.9755\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0410 - acc: 0.9866 - val_loss: 0.0755 - val_acc: 0.9769\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0403 - acc: 0.9875 - val_loss: 0.0800 - val_acc: 0.9755\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0385 - acc: 0.9880 - val_loss: 0.0787 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0333 - acc: 0.9901 - val_loss: 0.0727 - val_acc: 0.9778\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0326 - acc: 0.9907 - val_loss: 0.0731 - val_acc: 0.9778\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0322 - acc: 0.9908 - val_loss: 0.0727 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.0310 - acc: 0.9915 - val_loss: 0.0726 - val_acc: 0.9778\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.0309 - acc: 0.9915 - val_loss: 0.0725 - val_acc: 0.9777\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0308 - acc: 0.9916 - val_loss: 0.0723 - val_acc: 0.9775\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.0307 - acc: 0.9915 - val_loss: 0.0724 - val_acc: 0.9776\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.0307 - acc: 0.9918 - val_loss: 0.0723 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.0304 - acc: 0.9918 - val_loss: 0.0723 - val_acc: 0.9779\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.0304 - acc: 0.9918 - val_loss: 0.0724 - val_acc: 0.9779\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.0304 - acc: 0.9919 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.0303 - acc: 0.9919 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.0303 - acc: 0.9919 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.0303 - acc: 0.9920 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0303 - acc: 0.9920 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.0303 - acc: 0.9920 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "Epoch 00037: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the the model\n",
    "mt=model_c.fit(\n",
    "    train_features_flat,\n",
    "    train_label,\n",
    "    epochs=NB_EPOCHS,\n",
    "    validation_data=(val_features_flat, valid_label),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZWJOXjuORLgP",
    "outputId": "fba9f142-8b97-4876-e033-bdc88e5f66aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/step\n",
      "Test Accuracy:0.978800, Test Loss:0.067221.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "test_loss, test_acc = model_c.evaluate(test_features_flat, y_test_labels)\n",
    "print('Test Accuracy:%4f, Test Loss:%4f.' %(test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKojkQzJDpsW"
   },
   "outputs": [],
   "source": [
    "# Make Prediction\n",
    "pred_result = model_c.predict(test_features_flat)\n",
    "y_pred=[]\n",
    "for i in range(np.shape(y_test)[0]):\n",
    "    num = np.where(pred_result[i]==max(pred_result[i]))\n",
    "    y_pred.append(num[0][0])\n",
    "y_pred = np.transpose(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "GDuhH3tRG7HQ",
    "outputId": "1937f23c-99a2-4885-ff79-d36799170dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9788 / 10000 correct\n",
      "Accuracy = 0.978800\n",
      "[[ 969    0    1    0    0    4    5    1    0    0]\n",
      " [   0 1123    0    0    3    0    4    4    1    0]\n",
      " [   1    1 1008    4    2    4    4    5    2    1]\n",
      " [   0    0   10  977    0   16    0    3    2    2]\n",
      " [   0    1    1    0  967    1    2    3    4    3]\n",
      " [   2    0    7    9    1  864    0    2    6    1]\n",
      " [   7    0    4    0    0    2  944    0    1    0]\n",
      " [   0    5    4    2    8    0    0 1004    1    4]\n",
      " [   0    0    6    2    3    4    1    1  956    1]\n",
      " [   3    0    3    2    7    2    0    5   11  976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.989     0.988       980\n",
      "           1      0.994     0.989     0.992      1135\n",
      "           2      0.966     0.977     0.971      1032\n",
      "           3      0.981     0.967     0.974      1010\n",
      "           4      0.976     0.985     0.980       982\n",
      "           5      0.963     0.969     0.966       892\n",
      "           6      0.983     0.985     0.984       958\n",
      "           7      0.977     0.977     0.977      1028\n",
      "           8      0.972     0.982     0.977       974\n",
      "           9      0.988     0.967     0.977      1009\n",
      "\n",
      "    accuracy                          0.979     10000\n",
      "   macro avg      0.979     0.979     0.979     10000\n",
      "weighted avg      0.979     0.979     0.979     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvL9ICVYF1xQ"
   },
   "source": [
    "## Freeze All The Convolutional Layers Retrain VGG-16 Network On MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "7CDSVgEHFUm2",
    "outputId": "caaa52f8-6c1c-4499-a3e3-d01a1021cae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 34, 34, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 34, 34, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 17, 17, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  Create base model of VGG16\n",
    "from keras.applications import VGG16;\n",
    "from keras.layers import Input\n",
    "input_tensor = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH))\n",
    "freeze = VGG16(weights='imagenet',include_top=False, \n",
    "                  input_tensor = input_tensor)\n",
    "freeze.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "S4hLYyMuGjQK",
    "outputId": "3557a955-746e-4c49-de8c-c453b1034d76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    for layer in freeze.layers[3:4]:\\n    layer.trainable = False\\nfor layer in freeze.layers[6:8]:\\n    layer.trainable = False\\nfor layer in freeze.layers[10:12]:\\n    layer.trainable = False\\nfor layer in freeze.layers[14:16]:\\n    layer.trainable = False\\n    '"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in freeze.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8FHZUtDgFUpd",
    "outputId": "b6c0b313-7eaf-4742-ef2f-12e7b572161e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 18s 383us/step\n",
      "10000/10000 [==============================] - 4s 383us/step\n",
      "12000/12000 [==============================] - 5s 383us/step\n"
     ]
    }
   ],
   "source": [
    "# Extracting features\n",
    "train_features = freeze.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "test_features = freeze.predict(np.array(X_te), batch_size=BATCH_SIZE, verbose=1)\n",
    "val_features = freeze.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQw3v3OsFUsN"
   },
   "outputs": [],
   "source": [
    "np.savez(\"train_features\", train_features, train_label)\n",
    "np.savez(\"test_features\", test_features, y_test_labels)\n",
    "np.savez(\"val_features\", val_features, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LGdqVpHSHGzi",
    "outputId": "68a7b6cf-bf49-4e70-b1cc-fcdb0c1998a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 1, 1, 512) \n",
      " (10000, 1, 1, 512) \n",
      " (12000, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "# Current shape of features\n",
    "print(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBymV1m_HG6e"
   },
   "outputs": [],
   "source": [
    "# Flatten extracted features\n",
    "train_features_flat = np.reshape(train_features, (48000, 1*1*512))\n",
    "test_features_flat = np.reshape(test_features, (10000, 1*1*512))\n",
    "val_features_flat = np.reshape(val_features, (12000, 1*1*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "safIsZ7lHG9Y"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "frz_con = models.Sequential()\n",
    "frz_con.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
    "frz_con.add(layers.LeakyReLU(alpha=0.1))\n",
    "frz_con.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
    "frz_con.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bS6hUX4cHRfg"
   },
   "outputs": [],
   "source": [
    "NB_TRAIN_SAMPLES = train_features_flat.shape[0]\n",
    "NB_VALIDATION_SAMPLES = val_features_flat.shape[0]\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "# Compile the model.\n",
    "frz_con.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "YnHQV8z3Hcc8",
    "outputId": "628d9820-c440-40dd-e3e6-e902f4e55e54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# Incorporating reduced learning and early stopping for callback\n",
    "reduce_learning = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    epsilon=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=0)\n",
    "\n",
    "eary_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=8,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1105
    },
    "colab_type": "code",
    "id": "HKh91PqDHRiR",
    "outputId": "a7843668-23f6-4998-bbb2-ee7578a62bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.3069 - acc: 0.8986 - val_loss: 0.1590 - val_acc: 0.9487\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.1606 - acc: 0.9480 - val_loss: 0.1617 - val_acc: 0.9493\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.1342 - acc: 0.9561 - val_loss: 0.1472 - val_acc: 0.9514\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.1205 - acc: 0.9593 - val_loss: 0.1121 - val_acc: 0.9638\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.1077 - acc: 0.9648 - val_loss: 0.1392 - val_acc: 0.9549\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0987 - acc: 0.9674 - val_loss: 0.1204 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0571 - acc: 0.9815 - val_loss: 0.0839 - val_acc: 0.9744\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.0503 - acc: 0.9838 - val_loss: 0.0805 - val_acc: 0.9740\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0476 - acc: 0.9844 - val_loss: 0.0777 - val_acc: 0.9758\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0447 - acc: 0.9853 - val_loss: 0.0751 - val_acc: 0.9767\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0409 - acc: 0.9861 - val_loss: 0.0848 - val_acc: 0.9737\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0387 - acc: 0.9874 - val_loss: 0.0788 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0288 - acc: 0.9914 - val_loss: 0.0727 - val_acc: 0.9782\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0271 - acc: 0.9917 - val_loss: 0.0731 - val_acc: 0.9778\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0264 - acc: 0.9920 - val_loss: 0.0734 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.0243 - acc: 0.9933 - val_loss: 0.0725 - val_acc: 0.9781\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0239 - acc: 0.9933 - val_loss: 0.0722 - val_acc: 0.9781\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0727 - val_acc: 0.9783\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.0236 - acc: 0.9934 - val_loss: 0.0726 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0725 - val_acc: 0.9784\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.0725 - val_acc: 0.9783\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0230 - acc: 0.9937 - val_loss: 0.0725 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.0229 - acc: 0.9938 - val_loss: 0.0725 - val_acc: 0.9782\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0229 - acc: 0.9938 - val_loss: 0.0726 - val_acc: 0.9782\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.0726 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the the model\n",
    "mt=frz_con.fit(\n",
    "    train_features_flat,\n",
    "    train_label,\n",
    "    epochs=NB_EPOCHS,\n",
    "    validation_data=(val_features_flat, valid_label),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "chMdMllKHmcW",
    "outputId": "00b92aab-44c6-4c52-c7c6-3d72910e029f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/step\n",
      "Test Accuracy:0.977100, Test Loss:0.069289.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "test_loss, test_acc = frz_con.evaluate(test_features_flat, y_test_labels)\n",
    "print('Test Accuracy:%4f, Test Loss:%4f.' %(test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKz-CSaLH5Se"
   },
   "outputs": [],
   "source": [
    "# Make Prediction\n",
    "pred_result = frz_con.predict(test_features_flat)\n",
    "y_pred=[]\n",
    "for i in range(np.shape(y_test)[0]):\n",
    "  num = np.where(pred_result[i]==max(pred_result[i]))\n",
    "  y_pred.append(num[0][0])\n",
    "y_pred = np.transpose(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "Sdwr-Wq_H5ag",
    "outputId": "87b7d31b-9bce-4cf1-b93e-c6cd7e1c6d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9771 / 10000 correct\n",
      "Accuracy = 0.977100\n",
      "[[ 967    0    1    0    0    3    5    1    2    1]\n",
      " [   0 1125    0    0    3    0    4    2    1    0]\n",
      " [   1    2 1000    8    2    6    3    6    3    1]\n",
      " [   0    0    9  976    0   18    0    4    2    1]\n",
      " [   0    1    0    0  968    1    3    2    3    4]\n",
      " [   2    0    4   13    1  861    3    2    5    1]\n",
      " [   7    1    2    0    1    3  941    0    2    1]\n",
      " [   0    3    4    3   10    0    0 1002    1    5]\n",
      " [   0    0    3    4    2    4    2    2  955    2]\n",
      " [   3    0    3    3    5    3    0    4   12  976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.987     0.987       980\n",
      "           1      0.994     0.991     0.993      1135\n",
      "           2      0.975     0.969     0.972      1032\n",
      "           3      0.969     0.966     0.968      1010\n",
      "           4      0.976     0.986     0.981       982\n",
      "           5      0.958     0.965     0.961       892\n",
      "           6      0.979     0.982     0.981       958\n",
      "           7      0.978     0.975     0.976      1028\n",
      "           8      0.969     0.980     0.974       974\n",
      "           9      0.984     0.967     0.976      1009\n",
      "\n",
      "    accuracy                          0.977     10000\n",
      "   macro avg      0.977     0.977     0.977     10000\n",
      "weighted avg      0.977     0.977     0.977     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcNuxQeNNWWA"
   },
   "source": [
    "## Freeze All The Full Connected Layers Retrain VGG-16 Network On MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "35BsxuveNWky",
    "outputId": "edb36226-ec5d-4330-b321-5800f253c67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 34, 34, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 34, 34, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 17, 17, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  Create base model of VGG16\n",
    "from keras.applications import VGG16;\n",
    "from keras.layers import Input\n",
    "input_tensor = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH))\n",
    "freeze = VGG16(weights='imagenet',include_top=False, \n",
    "                  input_tensor = input_tensor)\n",
    "freeze.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hmEY1sylNWn_",
    "outputId": "1943f097-6ac9-41e5-9782-d31607e34a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 19s 401us/step\n",
      "10000/10000 [==============================] - 4s 400us/step\n",
      "12000/12000 [==============================] - 5s 403us/step\n"
     ]
    }
   ],
   "source": [
    "# Extracting features\n",
    "train_features = freeze.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "test_features = freeze.predict(np.array(X_te), batch_size=BATCH_SIZE, verbose=1)\n",
    "val_features = freeze.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Rab-CvhNWrB"
   },
   "outputs": [],
   "source": [
    "np.savez(\"train_features\", train_features, train_label)\n",
    "np.savez(\"test_features\", test_features, y_test_labels)\n",
    "np.savez(\"val_features\", val_features, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7ZZKMHLFN82Q",
    "outputId": "7d27bf86-44db-4dfb-dc68-c6f9d53f82c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 1, 1, 512) \n",
      " (10000, 1, 1, 512) \n",
      " (12000, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "# Current shape of features\n",
    "print(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAU1RjH-OHai"
   },
   "outputs": [],
   "source": [
    "# Flatten extracted features\n",
    "train_features_flat = np.reshape(train_features, (48000, 1*1*512))\n",
    "test_features_flat = np.reshape(test_features, (10000, 1*1*512))\n",
    "val_features_flat = np.reshape(val_features, (12000, 1*1*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G21XcJmN85v"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "frz_fcn = models.Sequential()\n",
    "frz_fcn.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
    "frz_con.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
    "frz_fcn.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOGX-uZdN9CT"
   },
   "outputs": [],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in frz_fcn.layers[0:2]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9jGIMoHORCm"
   },
   "outputs": [],
   "source": [
    "NB_TRAIN_SAMPLES = train_features_flat.shape[0]\n",
    "NB_VALIDATION_SAMPLES = val_features_flat.shape[0]\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "# Compile the model.\n",
    "\n",
    "frz_fcn.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "o-6d9pCuORH4",
    "outputId": "92f88569-c208-438b-e40b-bde0111b5cf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# Incorporating reduced learning and early stopping for callback\n",
    "reduce_learning = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    epsilon=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=0)\n",
    "\n",
    "eary_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=8,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "j0PoZPPzPVzZ",
    "outputId": "9d615663-1317-4131-e157-a072ca5c96a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 2.4916 - acc: 0.1330 - val_loss: 2.5009 - val_acc: 0.1352\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the the model\n",
    "mt=frz_fcn.fit(\n",
    "    train_features_flat,\n",
    "    train_label,\n",
    "    epochs=NB_EPOCHS,\n",
    "    validation_data=(val_features_flat, valid_label),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mxFjevJePXMj",
    "outputId": "c2cef3d6-90c1-4996-b464-4508e91a7272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 43us/step\n",
      "Test Accuracy:0.129600, Test Loss:2.491812.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "test_loss, test_acc = frz_fcn.evaluate(test_features_flat, y_test_labels)\n",
    "print('Test Accuracy:%4f, Test Loss:%4f.' %(test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5APCa3hfPXUm"
   },
   "outputs": [],
   "source": [
    "# Make Prediction\n",
    "pred_result = frz_fcn.predict(test_features_flat)\n",
    "y_pred=[]\n",
    "for i in range(np.shape(y_test)[0]):\n",
    "  num = np.where(pred_result[i]==max(pred_result[i]))\n",
    "  y_pred.append(num[0][0])\n",
    "y_pred = np.transpose(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "kGR9Wt2NPV2m",
    "outputId": "3c834ae6-49d1-4631-b0d4-0e712419c62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1296 / 10000 correct\n",
      "Accuracy = 0.129600\n",
      "[[   0    0    0    0    0    0    0    0  980    0]\n",
      " [   0    0    0    4  821  266    0    2   34    8]\n",
      " [   0    0    0    0    5    0    0   10 1015    2]\n",
      " [   0    0    0    0    4    0    0    0 1005    1]\n",
      " [   0    0    0    5  315   27    0   12  612   11]\n",
      " [   0    0    0    0    8    0    0    2  881    1]\n",
      " [   0    0    0    0    8    1    0    3  939    7]\n",
      " [   0    0    0    0  217   48    0    8  702   53]\n",
      " [   0    0    0    0    2    0    0    0  972    0]\n",
      " [   0    0    0    0   15    5    0    0  988    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       980\n",
      "           1      0.000     0.000     0.000      1135\n",
      "           2      0.000     0.000     0.000      1032\n",
      "           3      0.000     0.000     0.000      1010\n",
      "           4      0.226     0.321     0.265       982\n",
      "           5      0.000     0.000     0.000       892\n",
      "           6      0.000     0.000     0.000       958\n",
      "           7      0.216     0.008     0.015      1028\n",
      "           8      0.120     0.998     0.214       974\n",
      "           9      0.012     0.001     0.002      1009\n",
      "\n",
      "    accuracy                          0.130     10000\n",
      "   macro avg      0.057     0.133     0.050     10000\n",
      "weighted avg      0.057     0.130     0.049     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "num_test = len(y_test)\n",
    "num_correct = np.sum(y_pred == y_test)\n",
    "print('Got %d / %d correct' % (num_correct, num_test))\n",
    "print('Accuracy = %f' % (np.mean(y_test == y_pred)))\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=list(label_dict.values()),digits=3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
